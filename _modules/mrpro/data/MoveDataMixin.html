

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>mrpro.data.MoveDataMixin &mdash; mrpro</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=16f952a1" />

  
      <script src="../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            mrpro
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributor_guide.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../faq.html">FAQ</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">mrpro</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">mrpro.data.MoveDataMixin</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for mrpro.data.MoveDataMixin</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;MoveDataMixin.&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">dataclasses</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="kn">import</span> <span class="n">Iterator</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">copy</span> <span class="k">as</span> <span class="n">shallowcopy</span>
<span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">ClassVar</span><span class="p">,</span> <span class="n">Protocol</span><span class="p">,</span> <span class="n">Self</span><span class="p">,</span> <span class="n">TypeAlias</span><span class="p">,</span> <span class="n">overload</span><span class="p">,</span> <span class="n">runtime_checkable</span>

<span class="kn">import</span> <span class="nn">torch</span>


<span class="k">class</span> <span class="nc">InconsistentDeviceError</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>  <span class="c1"># noqa: D101</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">devices</span><span class="p">):</span>  <span class="c1"># noqa: D107</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Inconsistent devices found, found at least </span><span class="si">{</span><span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">devices</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>


<span class="nd">@runtime_checkable</span>
<span class="k">class</span> <span class="nc">DataclassInstance</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An instance of a dataclass.&quot;&quot;&quot;</span>

    <span class="n">__dataclass_fields__</span><span class="p">:</span> <span class="n">ClassVar</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">Field</span><span class="p">[</span><span class="n">Any</span><span class="p">]]]</span>


<div class="viewcode-block" id="MoveDataMixin">
<a class="viewcode-back" href="../../../_autosummary/mrpro.data.MoveDataMixin.html#mrpro.data.MoveDataMixin">[docs]</a>
<span class="k">class</span> <span class="nc">MoveDataMixin</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Move dataclass fields to cpu/gpu and convert dtypes.&quot;&quot;&quot;</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span> <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span> <span class="o">...</span>

    <span class="nd">@overload</span>
    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span> <span class="o">...</span>

<div class="viewcode-block" id="MoveDataMixin.to">
<a class="viewcode-back" href="../../../_autosummary/mrpro.data.MoveDataMixin.html#mrpro.data.MoveDataMixin.to">[docs]</a>
    <span class="k">def</span> <span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Perform dtype and/or device conversion of data.</span>

<span class="sd">        A torch.dtype and torch.device are inferred from the arguments</span>
<span class="sd">        args and kwargs. Please have a look at the</span>
<span class="sd">        documentation of torch.Tensor.to() for more details.</span>

<span class="sd">        A new instance of the dataclass will be returned.</span>

<span class="sd">        The conversion will be applied to all Tensor- or Module</span>
<span class="sd">        fields of the dataclass, and to all fields that implement</span>
<span class="sd">        the MoveDataMixin.</span>

<span class="sd">        The dtype-type, i.e. float or complex will always be preserved,</span>
<span class="sd">        but the precision of floating point dtypes might be changed.</span>

<span class="sd">        Example:</span>
<span class="sd">        If called with dtype=torch.float32 OR dtype=torch.complex64:</span>

<span class="sd">        - A complex128 tensor will be converted to complex64</span>
<span class="sd">        - A float64 tensor will be converted to float32</span>
<span class="sd">        - A bool tensor will remain bool</span>
<span class="sd">        - An int64 tensor will remain int64</span>

<span class="sd">        If other conversions are desired, please use the torch.Tensor.to() method of</span>
<span class="sd">        the fields directly.</span>

<span class="sd">        If the copy argument is set to True (default), a deep copy will be returned</span>
<span class="sd">        even if no conversion is necessary.</span>
<span class="sd">        If two fields are views of the same data before, in the result they will be independent</span>
<span class="sd">        copies if copy is set to True or a conversion is necessary.</span>
<span class="sd">        If set to False, some Tensors might be shared between the original and the new object.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Parse the arguments of the three overloads and call _to with the parsed arguments</span>
        <span class="n">parsedType</span><span class="p">:</span> <span class="n">TypeAlias</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">[</span>
            <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span>
        <span class="p">]</span>

        <span class="k">def</span> <span class="nf">parse3</span><span class="p">(</span>
            <span class="n">other</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">parsedType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">other</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">other</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span>

        <span class="k">def</span> <span class="nf">parse2</span><span class="p">(</span>
            <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">parsedType</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">memory_format</span>

        <span class="k">def</span> <span class="nf">parse1</span><span class="p">(</span>
            <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">dtype</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
            <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">parsedType</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">memory_format</span>

        <span class="k">if</span> <span class="n">args</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="s1">&#39;tensor&#39;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="c1"># overload 3 (&quot;tensor&quot; specifies the dtype and device)</span>
            <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">memory_format</span> <span class="o">=</span> <span class="n">parse3</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">args</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">):</span>
            <span class="c1"># overload 2 (no device specified, only dtype)</span>
            <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">memory_format</span> <span class="o">=</span> <span class="n">parse2</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># overload 1 (device and dtype specified)</span>
            <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">,</span> <span class="n">copy</span><span class="p">,</span> <span class="n">memory_format</span> <span class="o">=</span> <span class="n">parse1</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">memory_format</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span></div>


    <span class="k">def</span> <span class="nf">_items</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return an iterator over fields, parameters, buffers, and modules of the object.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">DataclassInstance</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">dataclasses</span><span class="o">.</span><span class="n">fields</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                <span class="n">name</span> <span class="o">=</span> <span class="n">field</span><span class="o">.</span><span class="n">name</span>
                <span class="n">data</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
                <span class="k">yield</span> <span class="n">name</span><span class="p">,</span> <span class="n">data</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
            <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">_parameters</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="k">yield from</span> <span class="bp">self</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_to</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">,</span>
        <span class="n">shared_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">memo</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
        <span class="n">new</span> <span class="o">=</span> <span class="n">shallowcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="k">if</span> <span class="n">copy</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move data to device and convert dtype if necessary.</span>

<span class="sd">        This method is called by .to(),  .cuda(),  .cpu(), .double(), and so on.</span>
<span class="sd">        It should not be called directly.</span>

<span class="sd">        See .to() for more details.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        device</span>
<span class="sd">            The destination device.</span>
<span class="sd">        dtype</span>
<span class="sd">            The destination dtype.</span>
<span class="sd">        non_blocking</span>
<span class="sd">            If True and the source is in pinned memory, the copy will be asynchronous with respect to the host.</span>
<span class="sd">            Otherwise, the argument has no effect.</span>
<span class="sd">        memory_format</span>
<span class="sd">            The desired memory format of returned tensor.</span>
<span class="sd">        shared_memory</span>
<span class="sd">            If True and the target device is CPU, the tensors will reside in shared memory.</span>
<span class="sd">            Otherwise, the argument has no effect.</span>
<span class="sd">        copy</span>
<span class="sd">            If True, the returned tensor will always be a copy, even if the input was already on the correct device.</span>
<span class="sd">            This will also create new tensors for views</span>
<span class="sd">        memo</span>
<span class="sd">            A dictionary to keep track of already converted objects to avoid multiple conversions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">memo</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">memo</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">def</span> <span class="nf">_tensor_to</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Move tensor to device and convert dtype if necessary.&quot;&quot;&quot;</span>
            <span class="n">new_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">|</span> <span class="kc">None</span>
            <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">:</span>
                <span class="n">new_dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="o">.</span><span class="n">to_real</span><span class="p">()</span>
            <span class="k">elif</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">data</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
                <span class="n">new_dtype</span> <span class="o">=</span> <span class="n">dtype</span><span class="o">.</span><span class="n">to_complex</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># bool or int: keep as is</span>
                <span class="n">new_dtype</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                <span class="n">device</span><span class="p">,</span>
                <span class="n">new_dtype</span><span class="p">,</span>
                <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">,</span>
                <span class="n">memory_format</span><span class="o">=</span><span class="n">memory_format</span><span class="p">,</span>
                <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">shared_memory</span><span class="p">:</span>
                <span class="n">data</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">data</span>

        <span class="k">def</span> <span class="nf">_module_to</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">copy</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">data</span><span class="o">.</span><span class="n">_apply</span><span class="p">(</span><span class="n">_tensor_to</span><span class="p">,</span> <span class="n">recurse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_mixin_to</span><span class="p">(</span><span class="n">obj</span><span class="p">:</span> <span class="n">MoveDataMixin</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MoveDataMixin</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">obj</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span>
                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">,</span>
                <span class="n">memory_format</span><span class="o">=</span><span class="n">memory_format</span><span class="p">,</span>
                <span class="n">shared_memory</span><span class="o">=</span><span class="n">shared_memory</span><span class="p">,</span>
                <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">,</span>
                <span class="n">memo</span><span class="o">=</span><span class="n">memo</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">converted</span><span class="p">:</span> <span class="n">Any</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">new</span><span class="o">.</span><span class="n">_items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
                <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">new</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">data</span><span class="p">)])</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">converted</span> <span class="o">=</span> <span class="n">_tensor_to</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">MoveDataMixin</span><span class="p">):</span>
                <span class="n">converted</span> <span class="o">=</span> <span class="n">_mixin_to</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
                <span class="n">converted</span> <span class="o">=</span> <span class="n">_module_to</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">copy</span><span class="p">:</span>
                <span class="n">converted</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">converted</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span> <span class="o">=</span> <span class="n">converted</span>
            <span class="c1"># this works even if new is frozen</span>
            <span class="nb">object</span><span class="o">.</span><span class="fm">__setattr__</span><span class="p">(</span><span class="n">new</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">converted</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new</span>

<div class="viewcode-block" id="MoveDataMixin.cuda">
<a class="viewcode-back" href="../../../_autosummary/mrpro.data.MoveDataMixin.html#mrpro.data.MoveDataMixin.cuda">[docs]</a>
    <span class="k">def</span> <span class="nf">cuda</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="nb">str</span> <span class="o">|</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">,</span>
        <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Put object in CUDA memory.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        device</span>
<span class="sd">            The destination GPU device. Defaults to the current CUDA device.</span>
<span class="sd">        non_blocking</span>
<span class="sd">            If True and the source is in pinned memory, the copy will be asynchronous with respect to the host.</span>
<span class="sd">            Otherwise, the argument has no effect.</span>
<span class="sd">        memory_format</span>
<span class="sd">            The desired memory format of returned tensor.</span>
<span class="sd">        copy:</span>
<span class="sd">            If True, the returned tensor will always be a copy, even if the input was already on the correct device.</span>
<span class="sd">            This will also create new tensors for views</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">memory_format</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span></div>


<div class="viewcode-block" id="MoveDataMixin.cpu">
<a class="viewcode-back" href="../../../_autosummary/mrpro.data.MoveDataMixin.html#mrpro.data.MoveDataMixin.cpu">[docs]</a>
    <span class="k">def</span> <span class="nf">cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">,</span> <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Put in CPU memory.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        memory_format</span>
<span class="sd">            The desired memory format of returned tensor.</span>
<span class="sd">        copy</span>
<span class="sd">            If True, the returned tensor will always be a copy, even if the input was already on the correct device.</span>
<span class="sd">            This will also create new tensors for views</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">memory_format</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span></div>


<div class="viewcode-block" id="MoveDataMixin.double">
<a class="viewcode-back" href="../../../_autosummary/mrpro.data.MoveDataMixin.html#mrpro.data.MoveDataMixin.double">[docs]</a>
    <span class="k">def</span> <span class="nf">double</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">,</span> <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert all float tensors to double precision.</span>

<span class="sd">        converts float to float64 and complex to complex128</span>


<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        memory_format</span>
<span class="sd">            The desired memory format of returned tensor.</span>
<span class="sd">        copy</span>
<span class="sd">            If True, the returned tensor will always be a copy, even if the input was already on the correct device.</span>
<span class="sd">            This will also create new tensors for views</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">memory_format</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span></div>


<div class="viewcode-block" id="MoveDataMixin.half">
<a class="viewcode-back" href="../../../_autosummary/mrpro.data.MoveDataMixin.html#mrpro.data.MoveDataMixin.half">[docs]</a>
    <span class="k">def</span> <span class="nf">half</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">,</span> <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert all float tensors to half precision.</span>

<span class="sd">        converts float to float16 and complex to complex32</span>


<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        memory_format</span>
<span class="sd">            The desired memory format of returned tensor.</span>
<span class="sd">        copy</span>
<span class="sd">            If True, the returned tensor will always be a copy, even if the input was already on the correct device.</span>
<span class="sd">            This will also create new tensors for views</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">memory_format</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span></div>


<div class="viewcode-block" id="MoveDataMixin.single">
<a class="viewcode-back" href="../../../_autosummary/mrpro.data.MoveDataMixin.html#mrpro.data.MoveDataMixin.single">[docs]</a>
    <span class="k">def</span> <span class="nf">single</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">memory_format</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">memory_format</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">,</span> <span class="n">copy</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convert all float tensors to single precision.</span>

<span class="sd">        converts float to float32 and complex to complex64</span>


<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        memory_format</span>
<span class="sd">            The desired memory format of returned tensor.</span>
<span class="sd">        copy</span>
<span class="sd">            If True, the returned tensor will always be a copy, even if the input was already on the correct device.</span>
<span class="sd">            This will also create new tensors for views</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">memory_format</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="n">copy</span><span class="p">)</span></div>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">device</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">|</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return the device of the tensors.</span>

<span class="sd">        Looks at each field of a dataclass implementing a device attribute,</span>
<span class="sd">        such as torch.Tensors or MoveDataMixin instances. If the devices</span>
<span class="sd">        of the fields differ, an InconsistentDeviceError is raised, otherwise</span>
<span class="sd">        the device is returned. If no field implements a device attribute,</span>
<span class="sd">        None is returned.</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        InconsistentDeviceError:</span>
<span class="sd">            If the devices of different fields differ.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">            The device of the fields or None if no field implements a device attribute.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_items</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;device&#39;</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="n">current_device</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="s1">&#39;device&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">current_device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">device</span> <span class="o">=</span> <span class="n">current_device</span>
            <span class="k">elif</span> <span class="n">device</span> <span class="o">!=</span> <span class="n">current_device</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">InconsistentDeviceError</span><span class="p">(</span><span class="n">current_device</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">device</span>

<div class="viewcode-block" id="MoveDataMixin.clone">
<a class="viewcode-back" href="../../../_autosummary/mrpro.data.MoveDataMixin.html#mrpro.data.MoveDataMixin.clone">[docs]</a>
    <span class="k">def</span> <span class="nf">clone</span><span class="p">(</span><span class="bp">self</span><span class="p">:</span> <span class="n">Self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Self</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a deep copy of the object.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_cuda</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return True if all tensors are on a single CUDA device.</span>

<span class="sd">        Checks all tensor attributes of the dataclass for their device,</span>
<span class="sd">        (recursively if an attribute is a MoveDataMixin)</span>


<span class="sd">        Returns False if not all tensors are on the same CUDA devices, or if the device is inconsistent,</span>
<span class="sd">        returns True if the data class has no tensors as attributes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="k">except</span> <span class="n">InconsistentDeviceError</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">is_cpu</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return True if all tensors are on the CPU.</span>

<span class="sd">        Checks all tensor attributes of the dataclass for their device,</span>
<span class="sd">        (recursively if an attribute is a MoveDataMixin)</span>

<span class="sd">        Returns False if not all tensors are on cpu or if the device is inconsistent,</span>
<span class="sd">        returns True if the data class has no tensors as attributes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">device</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span>
        <span class="k">except</span> <span class="n">InconsistentDeviceError</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">return</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;cpu&#39;</span></div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Physikalisch-Technische Bundesanstalt (PTB) Berlin.</p>
  </div>

   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>