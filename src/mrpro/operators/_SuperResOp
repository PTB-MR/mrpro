"""Class for Super Resolution Operator."""
#%%
# Copyright 2023 Physikalisch-Technische Bundesanstalt
#
# Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#       http://www.apache.org/licenses/LICENSE-2.0
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.

import itertools
from collections.abc import Callable
import matplotlib.pyplot as plt

import einops
import numpy as np
import torch
from scipy.spatial.transform import Rotation

from mrpro.operators import LinearOperator
#%%

class SuperResOp(LinearOperator):
    """Super resolution operator class."""

    def __init__(
        self,
        input_shape: tuple,
        R: Rotation,
        offset: torch.Tensor,
        w: int,
        slice_function: Callable,
        rotation_center=None,
    ):
        """Super resolution operator class.

        Generate a sparse matrix that represents the projection of a volume
        onto a plane that is determined by R and offset.

        Parameters
        ----------
        input_shape: tuple
            shape of a volume in (x,y,z)
        R: scipy.spatial.transform.Rotation
            Rotation that describes the orientation of the plane
        offset: torch.Tensor
            Offset of the plane in the volume
        w: int
            Factor that determines the number of pixels that are considered in the projection along the slice direction
        slice_function: Callable
            Function that describes the slice profile (weighting function, i.e. Gaussian)
        rotation_center: torch.Tensor
            Center of rotation, if None the center of the volume is used, i.e. for 4 pixels 0 1 2 3 it is between 1 and 2
        """

        self.projection_matrix = self._generate_matrix(
            input_shape=input_shape,
            R=R,
            offset=offset,
            w=w,
            slice_function=slice_function,
            rotation_center=rotation_center,
        )

    def _rotate(self, vector: torch.Tensor, R: Rotation, rotation_center=None, inverse=False):
        """Rotate tensor by scipy Rotation around a rotation center."""

        ret = vector.reshape(-1, 3)
        if rotation_center is not None:
            ret = ret - rotation_center
        ret = torch.tensor(R.apply(ret.numpy(), inverse)).float()
        if rotation_center is not None:
            ret = ret + rotation_center
        return ret.reshape(vector.shape)

    def _generate_matrix(
        self, input_shape: tuple, R: Rotation, offset: torch.Tensor, w: int, slice_function, rotation_center=None
    ):
        """Sample a sparse matrix that represents the projection of a volume
        onto a plane.

        Outside the volume it is zero padded.

        Returns
        -------
        torch.sparse_csr_matrix
            Sparse matrix that represents the projection operator of the volume onto the plane
        """

        X, Y, Z = input_shape

        # select slice position
        grid_xy = list(torch.meshgrid(torch.arange(X), torch.arange(Y)))
        grid_z = [Z / 2 * torch.ones(X, Y)]
        pixel = torch.stack(grid_xy + grid_z, dim=-1)
        pixel = pixel + offset

        if rotation_center is None:
            # default rotation center is the center of the volume, i.e. for 4 pixels
            # 0 1 2 3 it is between 0 and 1
            rotation_center = torch.tensor([X / 2 - 0.5, Y / 2 - 0.5, Z / 2 - 0.5])

        # rotate the slice
        pixel_rotated = self._rotate(pixel, R, rotation_center=rotation_center)

        # We cast a ray from the pixel perpendicular to the plane.
        # Pixels further away then w will not be considered
        ray = self._rotate(torch.stack([torch.zeros(2 * w), torch.zeros(2 * w), torch.arange(-w, w)], dim=-1), R)


        offsets = torch.tensor(list(itertools.product([0, 1], repeat=3)))

        # Distance between two pixels will always at least be 1
        # In all possible directions for each point along the line we consider the eight neighboring points
        # by adding all possible combinations of 0 and 1 to the point and flooring

        point = (
            pixel_rotated[:, :, None, None, :] + ray[None, None, None, :, :] + offsets[None, None, :, None, :]
        ).floor()
        distance = pixel_rotated[:, :, None, None, :] - point

        # Inverse rotation projects the distance back to the original coordinate system, i.e
        # Distance in z is distance along the line, i.e. the slice profile weighted direction
        # Distance in x and y is the distance of a pixel to the ray and linear interpolation is used to weight the distance
        distance_x, distance_y, distance_z = self._rotate(distance, R, inverse=True).unbind(-1)
        weight_xy = (1 - distance_x.abs()).clamp_min(0) * (1 - distance_y.abs()).clamp_min(0)
        weight_z = slice_function(distance_z)
        weight = (weight_xy * weight_z).reshape(X * Y, -1)

        # Mara: understand the next commented lines

        # Remove duplicates & points outside the volume
        # This is unfortunatly much slower than using torch.sparse.coo_tensor.coalesce later on
        # ids=torch.cat([torch.arange(len(source)).unsqueeze(1).expand(source.shape[:-1]).unsqueeze(-1),source],-1).reshape(-1,4)
        # _,inverse_idx,counts=torch.unique(ids,return_counts=True,return_inverse=True,dim=0)
        # counts = counts[inverse_idx].reshape(weight.shape).float()
        # weight = weight / counts.float()

        source = einops.rearrange(point, 'X Y s1 s2 xyz -> (X Y) (s1 s2) xyz').int()

        # Remove points outside the volume
        mask = (
            (source[..., 0] < X)
            & (source[..., 0] >= 0)
            & (source[..., 1] < Y)
            & (source[..., 1] >= 0)
            & (source[..., 2] < Z)
            & (source[..., 2] >= 0)
        )

        # Needed at the edge of the volume to approximate zero padding
        fraction_in_view = (mask * (weight > 0)).sum(-1) / (weight > 0).sum(-1)

        source_index = torch.tensor(np.ravel_multi_index(source[mask].unbind(-1), (X, Y, Z)))
        target_index = torch.repeat_interleave(torch.arange(X * Y), mask.sum(-1))

        # Count duplicates. Coalesce will sum the values of duplicate indices
        ones = torch.ones_like(source_index).float()
        ones = torch.sparse_coo_tensor(
            indices=torch.stack((target_index, source_index)), values=ones, size=(X * Y, X * Y * Z), dtype=torch.float32
        )
        ones = ones.coalesce()

        matrix = torch.sparse_coo_tensor(
            indices=torch.stack((target_index, source_index)),
            values=weight.reshape(X * Y, -1)[mask],
            size=(X * Y, X * Y * Z),
            dtype=torch.float32,
        ).coalesce()
        matrix.values()[:] /= ones.values()

        # Normalize
        norm = fraction_in_view / matrix.sum(1).to_dense()
        matrix *= norm[:, None]
        matrix = matrix.to_sparse_csr()

        return matrix

    def forward(self, img_hr: torch.Tensor) -> tuple[torch.Tensor,]:
        """Apply the projection operatore to high resolution 3d volume.

        Parameters
        ----------
        img_hr
            image data tensor with dimensions (other coils z y x)

        Returns
        -------
            image data tensor with dimensions (other coils 1 y x) that represents 2d low resolution slice
        """

        other, coils, z, y, x = img_hr.shape
        matrix = self.projection_matrix

        # couldnt find another way to broadcast sparse matrix
        flattened = einops.rearrange(img_hr, 'other coils z y x -> (other coils) (x y z)')
        slice = torch.stack([matrix @ flattened[i] for i in range(flattened.shape[0])], dim=0)
        return einops.rearrange(slice, '(other coils) (x y) -> other coils 1 y x', coils=coils, y=y)

    def adjoint(self, x: torch.Tensor) -> tuple[torch.Tensor,]:
        return super().adjoint(x)




# %%

class Settings_superRes:
    def __init__(self) -> None:
        self.R = Rotation.from_euler('xyz', [0, 0, 45], degrees=True)
        self.slice_center = torch.tensor([0, 0, 0])
        self.input_shape = (64, 64, 64)
        self.offset = torch.tensor([0, 0, 0])
        self.sigma = 1

    def sliceProf(self, distance_z):
        return torch.exp(-(distance_z**2) / (2 * self.sigma**2))

settings_super = Settings_superRes()

# %%
srr_op = SuperResOp(input_shape=settings_super.input_shape,
                    R=settings_super.R,
                    offset=settings_super.offset,
                    w=3 * settings_super.sigma,
                    slice_function=settings_super.sliceProf)
# %%


def simulHR(flag_show = True):
    img_square = torch.zeros(64, 64)
    img_square[22:42, 22:42] = 1

    square_vol = img_square.unsqueeze(0).repeat(64, 1, 1)

    if flag_show:
        plt.matshow(img_square)
    return square_vol
vol_HR = simulHR(flag_show = True)
#%%

vol_HR_exp = vol_HR.unsqueeze(0).unsqueeze(0).expand(-1, 2, -1, -1, -1)
slice_LR = srr_op.forward(vol_HR_exp)
print(slice_LR.shape)

plt.matshow(slice_LR[0, 0, 0, ...])
plt.show()
# %%
