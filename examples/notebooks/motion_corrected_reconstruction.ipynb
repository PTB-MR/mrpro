{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PTB-MR/mrpro/blob/main/examples/notebooks/motion_corrected_reconstruction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec('mrpro'):\n",
    "    %pip install mrpro[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Motion-corrected image reconstruction\n",
    "Physiological motion (e.g. due to breathing or the beating of the heart) is a common challenge for MRI. Motion during\n",
    "the data acquisition can lead to motion artifacts in the reconstructed image. Mathematically speaking, the motion\n",
    "occurring during data acquisition can be described by linear operators $M_m$ for each motion state $m$:\n",
    "\n",
    "$ y = \\sum_m A_m M_m x_{\\mathrm{true}} + n, $\n",
    "\n",
    "where $y$ is the acquired k-space data, $A_m$ is the acquisition operator describing which data was acquired in motion\n",
    "state $m$ and $n$ describes complex Gaussian noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "There are different approaches of how to minimize the impact of motion. The simplest approach is to acquire data only\n",
    "in a single motion state by e.g. asking the subject to hold their breath or synchronize the acquisition with the heart\n",
    "beat. This reduces the above problem to\n",
    "\n",
    "$ y = A x_{\\mathrm{true}} + n, $\n",
    "\n",
    "but this is not always possible and can also lead to long acquisition times.\n",
    "\n",
    "A more efficient approach is to acquire data in different motion states, estimate $M_m$ and solve the above problem.\n",
    "This is often referred to motion-corrected image reconstruction (MCIR). For some examples have a look at\n",
    "[Kolbitsch et al., JNM 2017](http://doi.org/10.2967/jnumed.115.171728),\n",
    "[Ippoliti et al., MRM 2019](http://doi.wiley.com/10.1002/mrm.27867) or\n",
    "[Mayer et al., JNM 2021](http://doi.org/10.1007/s00259-020-05180-4)\n",
    "\n",
    "Here we will show how to do a MCIR of a free-breathing acquisition of the thorax following these steps:\n",
    "1. Estimate a respiratory self-navigator from the acquired k-space data\n",
    "2. Use the self-navigator to separate the data into different breathing states\n",
    "3. Reconstruct dynamic images of different breathing states\n",
    "4. Estimate non-rigid motion fields from the dynamic images\n",
    "5. Use the motion fields to obtain a motion-corrected image\n",
    "\n",
    "To achieve high image quality a TV-regularized image reconstruction is used here. To safe time we will use only a few\n",
    "iterations. Increase 'n_iterations_tv' in the code to 100 to get a better image quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations_tv = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Data acquisition\n",
    "The data was acquired with a Golden Radial Phase Encoding (GRPE) sampling scheme\n",
    "[[Prieto et al., MRM 2010](http://doi.org/10.1002/mrm.22446)]. This sampling scheme combines a Cartesian readout with\n",
    "radial phase encoding in the 2D ky-kz plane. The central k-space line (i.e. 1D projection of the object along the\n",
    "foot-head direction) is acquired repeatedly and can be used as a respiratory self-navigator. This sequence was\n",
    "implemented in pulseq and also available as a seq-file. The FOV of the scan was 288 x 288 x 288 $mm^3$ with an\n",
    "isotropic resolution of 1.9 mm.\n",
    "```{note}\n",
    "To keep reconstruction times short, we used a short acquisition of less than one minute. We also only split the data\n",
    "into 4 motion states with rather large overlap (sliding-window) between motion states. For reliable motion correction\n",
    "at least 6 motion states should be used.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show download details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Download raw data and pre-calculated motion fields from zenodo into a temporary directory\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import zenodo_get\n",
    "from einops import rearrange\n",
    "from mrpro.algorithms.optimizers import cg, pdhg\n",
    "from mrpro.algorithms.reconstruction import IterativeSENSEReconstruction\n",
    "from mrpro.data import CsmData, IData, KData\n",
    "from mrpro.data.traj_calculators import KTrajectoryRpe\n",
    "from mrpro.operators import (\n",
    "    AveragingOp,\n",
    "    FastFourierOp,\n",
    "    FiniteDifferenceOp,\n",
    "    FourierOp,\n",
    "    GridSamplingOp,\n",
    "    LinearOperatorMatrix,\n",
    "    ProximableFunctionalSeparableSum,\n",
    "    SensitivityOp,\n",
    ")\n",
    "from mrpro.operators.functionals import L1NormViewAsReal, L2NormSquared, ZeroFunctional\n",
    "from mrpro.utils import unsqueeze_right\n",
    "\n",
    "dataset = '15288250'\n",
    "\n",
    "tmp = tempfile.TemporaryDirectory()  # RAII, automatically cleaned up\n",
    "data_folder = Path(tmp.name)\n",
    "zenodo_get.zenodo_get([dataset, '-r', 5, '-o', data_folder])  # r: retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tv_reg_reco(kdata, csm, img_initial, reg_weight=0.1, reg_weight_t=0.1, n_iterations=100):\n",
    "    fourier_operator = FourierOp.from_kdata(kdata)\n",
    "    csm_operator = SensitivityOp(csm)\n",
    "    acquisition_operator = fourier_operator @ csm_operator\n",
    "\n",
    "    if img_initial.data.shape[0] == 1:\n",
    "        tv_dim = (-3, -2, -1)\n",
    "        regularization_weight = torch.tensor([reg_weight, reg_weight, reg_weight])\n",
    "    else:\n",
    "        tv_dim = (-5, -3, -2, -1)\n",
    "        regularization_weight = torch.tensor([reg_weight_t, reg_weight, reg_weight, reg_weight])\n",
    "\n",
    "    nabla_operator = FiniteDifferenceOp(dim=tv_dim, mode='forward')\n",
    "    l2 = 0.5 * L2NormSquared(target=kdata.data)\n",
    "    l1 = L1NormViewAsReal(weight=unsqueeze_right(regularization_weight, kdata.data.ndim))\n",
    "\n",
    "    f = ProximableFunctionalSeparableSum(l2, l1)\n",
    "    g = ZeroFunctional()\n",
    "    K = LinearOperatorMatrix(((acquisition_operator,), (nabla_operator,)))\n",
    "\n",
    "    initial_values = (img_initial.data.clone(),)\n",
    "\n",
    "    (img_pdhg,) = pdhg(f=f, g=g, operator=K, initial_values=initial_values, max_iterations=n_iterations)\n",
    "    return IData(data=img_pdhg, header=img_initial.header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Motion-corrupted image reconstruction\n",
    "As a first step we will reconstruct the image using all the acquired data which will lead to an image corrupted by\n",
    "respiratory motion.\n",
    "```{note}\n",
    "To reduce the file size we have already applied coil compression reducing the 21 physical coils to 6 compressed coils.\n",
    "We also removed the readout oversampling.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdata = KData.from_file(data_folder / 'grpe_t1_free_breathing.mrd', KTrajectoryRpe(angle=torch.pi * 0.618034))\n",
    "\n",
    "# Calculate coil maps\n",
    "csm_maps = CsmData.from_kdata_inati(kdata, smoothing_width=3, downsampled_size=64)\n",
    "\n",
    "#  SENSE reconstruction\n",
    "iterative_sense = IterativeSENSEReconstruction(kdata, csm=csm_maps)\n",
    "img = iterative_sense.forward(kdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def show_views(image: torch.Tensor) -> None:\n",
    "    \"\"\"Plot coronal, transversal and sagittal view.\"\"\"\n",
    "    image = torch.squeeze(image / image.max())\n",
    "    image_views = [image[:, 92, :], torch.fliplr(image[:, :, 92]), image[100, :, :]]\n",
    "    _, axes = plt.subplots(1, 3, squeeze=False, figsize=(12, 6))\n",
    "    for idx, (view, title) in enumerate(zip(image_views, ['Coronal', 'Transversal', 'Sagittal'], strict=False)):\n",
    "        axes[0, idx].imshow(torch.rot90(view), vmin=0, vmax=0.4 if idx == 1 else 0.25, cmap='grey')\n",
    "        axes[0, idx].set_title(title, fontsize=18)\n",
    "        axes[0, idx].set_xticks([])\n",
    "        axes[0, idx].set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Visualize anatomical views of 3D image\n",
    "show_views(img.rss())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### 1. Respiratory self-navigator\n",
    "We are going to obtain a self-navigator from the k-space center line ky = kz = 0 following these steps:\n",
    "- Get all readout lines for ky = kz = 0\n",
    "- Apply a 1D FFT along the readout to get the projection of the object\n",
    "- Carry out SVD over all readout points and all coils to get the main signal components\n",
    "- Select the SVD component the largest frequency contribution between 0.2 and 0.5 Hz (realistic breathing frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show self-navigator calculation"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def get_respiratory_self_navigator_from_grpe(\n",
    "    kdata: KData, respiratory_frequency_range: tuple[float, float] = (0.2, 0.5)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Get respiratory self-navigator from GRPE data set.\"\"\"\n",
    "    # Get all readout lines for ky = kz = 0\n",
    "    ky0_kz0_idx = torch.where((kdata.traj.ky == 0) & (kdata.traj.kz == 0))\n",
    "    navigator_data = kdata.data[ky0_kz0_idx[0], :, ky0_kz0_idx[2], ky0_kz0_idx[3], :]\n",
    "    navigator_time = kdata.header.acq_info.acquisition_time_stamp[ky0_kz0_idx[0], 0, ky0_kz0_idx[2], ky0_kz0_idx[3], 0]\n",
    "\n",
    "    # Apply a 1D FFT along the readout to get the projection of the object\n",
    "    fft_op_1d = FastFourierOp(dim=(-1,))\n",
    "    navigator_data = torch.abs(fft_op_1d(navigator_data)[0])\n",
    "\n",
    "    # Carry out SVD over all readout points and all coils to get the main signal components\n",
    "    navigator_data = rearrange(navigator_data, 'k1k2 coil k0 -> k1k2 (coil k0)')\n",
    "    svd_navigator_data, _, _ = torch.linalg.svd(navigator_data - navigator_data.mean(dim=0, keepdim=True))\n",
    "\n",
    "    # Select the SVD component the largest frequency contribution closest to the expected respiratory frequency\n",
    "    dt = torch.mean(torch.diff(navigator_time, dim=0))\n",
    "    fft_svd_navigator_data = torch.abs(torch.fft.fft(svd_navigator_data, dim=0))\n",
    "    f_hz = torch.linspace(0, 1 / dt, svd_navigator_data.shape[0])\n",
    "    fft_svd_navigator_data_in_resp_window = fft_svd_navigator_data[\n",
    "        (f_hz >= respiratory_frequency_range[0]) & (f_hz <= respiratory_frequency_range[1]), :\n",
    "    ]\n",
    "    respiratory_navigator = svd_navigator_data[\n",
    "        :, torch.argmax(torch.max(fft_svd_navigator_data_in_resp_window, dim=0)[0])\n",
    "    ]\n",
    "\n",
    "    # Interpolate navigator from k-space center ky=kz=0 to all phase encoding points\n",
    "    return torch.as_tensor(\n",
    "        np.interp(kdata.header.acq_info.acquisition_time_stamp[0, 0, 0, :, 0], navigator_time, respiratory_navigator)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To separate all phase encoding points into different motion states we combine ky and kz points along k1 first\n",
    "kdata = kdata.rearrange('... k2 k1 k0 -> ... 1 (k2 k1) k0')\n",
    "\n",
    "respiratory_navigator = get_respiratory_self_navigator_from_grpe(kdata)\n",
    "\n",
    "plt.figure()\n",
    "acquisition_time = kdata.header.acq_info.acquisition_time_stamp[0, 0, 0, :, 0]\n",
    "plt.plot(acquisition_time - acquisition_time.min(), respiratory_navigator)\n",
    "plt.xlabel('Acquisition time (s)')\n",
    "plt.ylabel('Navigator signal (a.u.)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 2. Split data into different breathing states\n",
    "The self-navigator is used to split the data into different motion phases. We use a sliding window approach to ensure\n",
    "we have got enough data in each motion state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points_per_motion_state = int(kdata.data.shape[-2] * 0.36)\n",
    "navigator_idx = respiratory_navigator.argsort()\n",
    "navigator_idx = navigator_idx.unfold(0, n_points_per_motion_state, n_points_per_motion_state // 2)\n",
    "kdata_resp_resolved = kdata[..., navigator_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 3. Reconstruct dynamic images of different breathing states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_resp_resolved = IterativeSENSEReconstruction(kdata_resp_resolved, csm=csm_maps)\n",
    "img_resp_resolved = recon_resp_resolved.forward(kdata_resp_resolved)\n",
    "\n",
    "img_resp_resolved_tv = tv_reg_reco(\n",
    "    kdata_resp_resolved,\n",
    "    csm_maps,\n",
    "    img_initial=img_resp_resolved,\n",
    "    reg_weight=1e-7,\n",
    "    reg_weight_t=2e-6,\n",
    "    n_iterations=n_iterations_tv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def show_motion_states(*images: torch.Tensor, ylabels: list[str] | None = None, slice_idx: int = 92) -> None:\n",
    "    \"\"\"Plot first and last motion state and difference image.\"\"\"\n",
    "    n_images = len(images)\n",
    "    _, axes = plt.subplots(n_images, 3, squeeze=False, figsize=(n_images * 6, 8))\n",
    "    [a.set_xticks([]) for a in axes.flatten()]\n",
    "    [a.set_yticks([]) for a in axes.flatten()]\n",
    "    for i in range(n_images):\n",
    "        image = torch.squeeze(images[i] / images[i].max())\n",
    "        axes[i, 0].imshow(torch.rot90(image[0, :, slice_idx, :]), vmin=0, vmax=0.2, cmap='grey')\n",
    "        axes[i, 0].set_title('MS 1', fontsize=18)\n",
    "        axes[i, 0].set_ylabel(ylabels[i], fontsize=18)\n",
    "        axes[i, 1].imshow(torch.rot90(image[-1, :, slice_idx, :]), vmin=0, vmax=0.2, cmap='grey')\n",
    "        axes[i, 1].set_title(f'MS {image.shape[0]}', fontsize=18)\n",
    "        axes[i, 2].imshow(\n",
    "            torch.rot90(torch.abs(image[0, :, slice_idx, :] - image[-1, :, slice_idx, :])),\n",
    "            vmin=0,\n",
    "            vmax=0.2,\n",
    "            cmap='grey',\n",
    "        )\n",
    "        axes[i, 2].set_title(f'|MS 1 - MS {image.shape[0]}|', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_motion_states(\n",
    "    img_resp_resolved.rss(), img_resp_resolved_tv.rss(), ylabels=('Iterative SENSE', 'TV-regularization')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 4. Estimate the motion fields from the dynamic images\n",
    "The motion fields can be estimated with an image registration package such as\n",
    "[mirtk](https://mirtk.github.io/commands/register.html). Here we registered each of the dynamic respiratory phases to\n",
    "the first motion state using the free-form deformation (FFD) registration approach with a control point spacing of 9.\n",
    "Normalized mutual information was used as a similarity metric and a LogJac penalty weight of 0.001 was applied.\n",
    "\n",
    "We load the displacement fields and create a motion operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = torch.as_tensor(np.load(data_folder / 'grpe_t1_free_breathing_displacement_fields.npy'), dtype=torch.float32)\n",
    "motion_op = GridSamplingOp.from_displacement(mf[..., 2], mf[..., 1], mf[..., 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### 5. Use the motion fields to obtain a motion-corrected image\n",
    "Now we obtain a motion-corrected image $x$ by minimizing the functionl $F$\n",
    "\n",
    "$ F(x) = ||\\sum_m(A_mx - y_m)||_2^2 \\quad$ with $\\quad A_m = F_m M_m C $\n",
    "\n",
    "where $C$ describes the coil-sensitivity maps, $M_m$ is the motion transformation of motion state $m$ and $F_m$\n",
    "describes the Fourier transform of all of the k-space points obtained in motion state $m$.\n",
    "\n",
    "One way to solve this problem with MRpro is to use the respiratory-resolved k-space data from above. Because we used a\n",
    "sliding window approach to split the data into different motion states, this computationally a bit more demanding than\n",
    "it needs to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create acquisition operator\n",
    "fourier_op = recon_resp_resolved.fourier_op\n",
    "dcf_op = recon_resp_resolved.dcf.as_operator()\n",
    "csm_op = SensitivityOp(csm_maps)\n",
    "averaging_op = AveragingOp(dim=0)\n",
    "acquisition_operator = fourier_op @ motion_op @ csm_op @ averaging_op.H\n",
    "\n",
    "(initial_value,) = acquisition_operator.H(dcf_op(kdata_resp_resolved.data)[0])\n",
    "(right_hand_side,) = acquisition_operator.H(kdata_resp_resolved.data)\n",
    "operator = acquisition_operator.H @ acquisition_operator\n",
    "\n",
    "# Minimize the functional\n",
    "(img_mcir,) = cg(operator, right_hand_side, initial_value=initial_value, max_iterations=30, tolerance=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def show_images(*images: torch.Tensor, titles: list[str] | None = None) -> None:\n",
    "    \"\"\"Plot images.\"\"\"\n",
    "    n_images = len(images)\n",
    "    _, axes = plt.subplots(1, n_images, squeeze=False, figsize=(n_images * 3, 3))\n",
    "    for i in range(n_images):\n",
    "        image = torch.squeeze(images[i] / images[i].max())\n",
    "        axes[0][i].imshow(torch.rot90(image[:, 93, :]), cmap='gray', vmin=0, vmax=0.18)\n",
    "        axes[0][i].axis('off')\n",
    "        if titles:\n",
    "            axes[0][i].set_title(titles[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(img.rss(), img_mcir.abs(), titles=('Uncorrected', 'MCIR'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "mystnb,tags,-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
