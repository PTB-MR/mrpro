{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PTB-MR/mrpro/blob/main/examples/notebooks/learning_regularization_parameter_maps_for_tv_reconstruction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec('mrpro'):\n",
    "    %pip install mrpro[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Learning spatially adaptive regularization parameter maps for total-variation (TV)-minimization reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this notebook, we demonstrate how the method presented in\n",
    "[[Kofler et al, SIIMS 2023](https://epubs.siam.org/doi/abs/10.1137/23M1552486)] can be implemented for\n",
    "2D MR reconstruction problems using MRpro. The method consists of two main blocks.\n",
    "1) The first block is a neural network architecture that estimates spatially adaptive regularization\n",
    "parameter maps from an input image, which are then used in 2).\n",
    "2) The second block corresponds to an unrolled unrolled Primal-Dual Hybrid Gradient (PDHG) algorithm\n",
    "[[Chambolle \\& Pock, JMIV 2011](https://doi.org/10.1007%2Fs10851-010-0251-1)] that\n",
    "reconstructs the image from the undersampled k-space data measurements assuming the regularization\n",
    "parameter maps to be fixed.\n",
    "\n",
    "The entire network can be trained end-to-end using MRpro, allowing to learn to\n",
    "estimate spatially adaptive regularization parameter maps for TV from an input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The method\n",
    "In the TV-example, (see <project:tv_minimization_reconstruction_pdhg.ipynb>), you can see how to employ the primal\n",
    "dual hybrid gradient (PDHG - `mrpro.algorithms.optimizers.pdhg`) method\n",
    "[[Chambolle \\& Pock, JMIV 2011](https://doi.org/10.1007%2Fs10851-010-0251-1)]\n",
    "to solve the TV-minimization problem. For data acquired according to the usual model\n",
    "\n",
    "$ y = Ax_{\\mathrm{true}} + n, $\n",
    "\n",
    "where $A$ contains the Fourier transform, the coil sensitivity maps operator, etc, and $n$ is complex-valued\n",
    "Gaussian noise, the TV-minimization problem is given by\n",
    "\n",
    "$\\mathcal{F}_{\\lambda}(x) = \\frac{1}{2}||Ax - y||_2^2 + \\lambda \\| \\nabla x \\|_1, \\quad \\quad \\quad (1)$\n",
    "\n",
    "where $\\nabla$ is the discretized gradient operator and $\\lambda>0$ globally dictates the strength of the\n",
    "regularization. Clearly, having one global regularization parameter $\\lambda$ for the entire image is\n",
    "not optimal, as the image content can vary significantly across the image. Therefore, in this example,\n",
    "we aim to learn spatially adaptive regularization parameter maps $\\Lambda_{\\theta}$ from the input image to improve\n",
    "our TV-reconstruction. I.e., we are interested in estimating spatially adaptive regularization parameter maps by\n",
    "\n",
    "$\\Lambda_{\\theta}:=u_{\\theta}(x_0)$\n",
    "\n",
    "with a convolutional neural network $u_{\\theta}$ with trainable parameters $\\theta$ from an input image $x_0$,\n",
    "and then to consider the weighted TV-minimization problem\n",
    "\n",
    "$\\mathcal{F}_{\\Lambda_{\\theta}}(x) = \\frac{1}{2}||Ax - y||_2^2 +  \\| \\Lambda_{\\theta} \\nabla x \\|_1, \\quad \\quad (2)$\n",
    "\n",
    "where $\\Lambda_{\\theta}$ is voxel-wise strictly positive and locally regularizes the problem by\n",
    "differently weighting the gradient of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## The neural network for estimating the regularization parameter maps\n",
    "In this example, we use a simple convolutional neural network to estimate the regularization parameter maps.\n",
    "Obviously, more complex and sophisticated architectures can be employed as well. For example, in the work in\n",
    "[[Kofler et al, SIIMS 2023](https://epubs.siam.org/doi/abs/10.1137/23M1552486)],\n",
    "a U-Net [[Ronneberger et al, MICCAI 2015](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28)] was used.\n",
    "The network used here corresponds to a simple block of convolutional layers with leaky ReLU activations and a\n",
    "final softplus activation to ensure that the regularization parameter maps are strictly positive.\n",
    "The network is defined in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ParameterMapNetwork2D(torch.nn.Module):\n",
    "    r\"\"\"A simple network for estimating regularization parameter maps for TV-reconstruction.\"\"\"\n",
    "\n",
    "    def __init__(self, n_filters: int = 32) -> None:\n",
    "        r\"\"\"Initialize Adaptive TV Network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_filters\n",
    "            Number of filters to be applied in the convolutional layers of the network.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cnn_block: torch.nn.Module = torch.nn.Sequential(\n",
    "            *[\n",
    "                torch.nn.InstanceNorm2d(1),\n",
    "                torch.nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Conv2d(in_channels=n_filters, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.Softplus(beta=5.0),\n",
    "            ]\n",
    "        )\n",
    "        # raw parameter t; softplus is used to \"activate\" it and make it strictly positive\n",
    "        self.t = torch.nn.Parameter(torch.tensor([0.0], requires_grad=True))\n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Apply the network to estimate regularization parameter maps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image\n",
    "            the image from which the regularization parameter maps should be estimated.\n",
    "        \"\"\"\n",
    "        regularization_parameter_map = self.cnn_block(image)\n",
    "        regularization_parameter_map = torch.nn.functional.softplus(self.t, beta=5.0) * regularization_parameter_map\n",
    "        return regularization_parameter_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## The unrolled PDHG algorithm network\n",
    "We now construct the second block, i.e. network that (approximately) solves the TV-minimization problem with spatially\n",
    "adaptive regularization parameter maps given in (2) by unrolling a finite number of iterations of PDHG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "```{note}\n",
    "To fully understand the mechanism of the network, we recommend to first have a look at the TV-example in\n",
    "<project:tv_minimization_reconstruction_pdhg.ipynb>, especially if you are not familiar with the PDHG algorithm\n",
    "or how to use it to solve the TV-minimization problem.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Put in simple words, the network takes the initial image, estimates the regularization\n",
    "parameter maps, and then sets up the TV-problem described in (2) within the \"forward\" of the network.\n",
    "The network then approximately solves the TV-problem using the PDHG algorithm\n",
    "and returns the reconstructed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import mrpro\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class AdaptiveTVNetwork2D(torch.nn.Module):\n",
    "    r\"\"\"Unrolled primal dual hybrid gradient with spatially adaptive regularization parameter maps for TV.\n",
    "\n",
    "    Solves the minimization problem\n",
    "\n",
    "        :math:`\\min_x \\frac{1}{2}\\| Ax - y\\|_2^2 + \\| \\Lambda_{\\theta} \\nabla x\\|_1`,\n",
    "    where :math:`A` is the forward linear operator, :math:`\\nabla` is the gradient operator,\n",
    "    and :math:`\\Lambda_{\\theta}` is a strictly positive regularization parameter map that is estimated from\n",
    "    an input image with a network :math:`u_{\\theta}` with trainable parameters :math:`\\theta`.\n",
    "\n",
    "    N.B. The entire network sticks to the convention of MRpro, i.e. we work with images and k-space data\n",
    "    of shape (other*, coils, z, y, x). However, because here showcase the method for 2D problems,\n",
    "    some processing steps are necessary within the forward method. In particular, we restrict this example,\n",
    "    to be used with z=1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        recon_matrix: mrpro.data.SpatialDimension[int],\n",
    "        encoding_matrix: mrpro.data.SpatialDimension[int],\n",
    "        lambda_map_network: ParameterMapNetwork2D,\n",
    "        n_iterations: int = 128,\n",
    "    ):\n",
    "        r\"\"\"Initialize Adaptive TV Network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        recon_matrix\n",
    "            image shape, i.e. the shape of the domain of the fourier operator.\n",
    "        encoding_matrix\n",
    "           k-space shape, i.e. the shape of the domain of the fourier operator\n",
    "        lambda_map_network\n",
    "            a network that predicts a regularization parameter map from the input image.\n",
    "        n_iterations\n",
    "            number of iterations for the unrolled primal dual hybrid gradient (PDHG) algorithm.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.recon_matrix = recon_matrix\n",
    "        self.encoding_matrix = encoding_matrix\n",
    "\n",
    "        self.lambda_map_network = lambda_map_network\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        finite_differences_operator = mrpro.operators.FiniteDifferenceOp(dim=(-2, -1), mode='forward')\n",
    "        self.gradient_operator = (\n",
    "            mrpro.operators.RearrangeOp('grad batch ... -> batch grad ... ') @ finite_differences_operator\n",
    "        )\n",
    "        self.g = mrpro.operators.functionals.ZeroFunctional()\n",
    "\n",
    "        # operator norm of the stacked operator K=[A, \\nabla]^T\n",
    "        stacked_operator_norm = 3.0  # analytically calculated\n",
    "        self.primal_stepsize = self.dual_stepsize = 0.95 * (1.0 / stacked_operator_norm)\n",
    "\n",
    "    def estimate_lambda_map(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Estimate regularization parameter map from image.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image\n",
    "            coil combined 2d image with shape `batch, coils=1, z=1, y x`\n",
    "        \"\"\"\n",
    "        input_image = rearrange(image.abs(), '... 1 1 y x -> ... 1 y x')\n",
    "        regularization_parameter_map = self.lambda_map_network(input_image).unsqueeze(-3).unsqueeze(-3)\n",
    "        return regularization_parameter_map\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        initial_image: torch.Tensor,\n",
    "        kdata: torch.Tensor,\n",
    "        forward_operator: mrpro.operators.LinearOperator,\n",
    "        regularization_parameter: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Reconstruct image using an unrolled PDHG algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_image\n",
    "            initial guess of the solution of the TV problem.\n",
    "        kdata\n",
    "            k-space data tensor of the considered problem.\n",
    "        forward_operator\n",
    "            forward operator that maps the image to the k-space data.\n",
    "        regularization_parameter\n",
    "            regularization parameter to be used in the TV-functional. If set to None,\n",
    "            it is estimated by the lambda_map_network.\n",
    "            (can also be a single scalar)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Image reconstructed by the PDGH algorithm to solve the weighted TV problem.\n",
    "        \"\"\"\n",
    "        # if no regularization parameter map is provided, compute it with the network\n",
    "        if regularization_parameter is None:\n",
    "            regularization_parameter = self.estimate_lambda_map(initial_image)\n",
    "\n",
    "        l2_norm_squared = 0.5 * mrpro.operators.functionals.L2NormSquared(target=kdata)\n",
    "        l1_norm = mrpro.operators.functionals.L1NormViewAsReal(weight=regularization_parameter)\n",
    "        f = mrpro.operators.ProximableFunctionalSeparableSum(l2_norm_squared, l1_norm)\n",
    "\n",
    "        stacked_operator = mrpro.operators.LinearOperatorMatrix(((forward_operator,), (self.gradient_operator,)))\n",
    "\n",
    "        (solution,) = mrpro.algorithms.optimizers.pdhg(\n",
    "            f=f,\n",
    "            g=self.g,\n",
    "            operator=stacked_operator,\n",
    "            initial_values=(initial_image,),\n",
    "            max_iterations=self.n_iterations,\n",
    "            primal_stepsize=self.primal_stepsize,\n",
    "            dual_stepsize=self.dual_stepsize,\n",
    "            tolerance=0.0,\n",
    "        )\n",
    "        return solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Creating the training data\n",
    "In the following, we create some training data for the network. We use some images borrowed from the\n",
    "BrainWeb dataset [[Aubert-Broche et al, IEEE TMI 2006](https://ieeexplore.ieee.org/abstract/document/1717639)],\n",
    "which is a simulated MRI dataset and for which MRpro provides a simple interface to load the data, see\n",
    "`mrpro.phantoms.brainweb`.\n",
    "For simplicity, we here use some images that were generated by simulating the contrast using the\n",
    "inversion recovery signal model `mrpro.operators.models.InversionRecovery` in MRpro.\n",
    "In this notebook, we consider an ultra low field MR reconstruction problem, where the forward operator is given by\n",
    "$A:=S_I F$, where $F$ is a simple Cartesian Fourier operator with a single coil and $S_I$ corresponds to a mask\n",
    "only selecting the the central k-space region. Further, the present noise component is modeled to have a rather large\n",
    "variance. This means that in this setting, we tackle both the problem of super-resolution and denoising at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Download the data from Zenodo, create the datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show download and dataloading details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import zenodo_get\n",
    "\n",
    "dataset_tv_example = '15348115'\n",
    "\n",
    "tmp = tempfile.TemporaryDirectory()  # RAII, automatically cleaned up\n",
    "data_folder_tv = Path(tmp.name)\n",
    "zenodo_get.download(\n",
    "    record=dataset_tv_example,\n",
    "    retry_attempts=5,\n",
    "    output_dir=data_folder_tv,\n",
    "    file_glob=(\n",
    "        'tv_model.pt',\n",
    "        'training_images_part.pt',\n",
    "        'validation_images_part.pt',\n",
    "        'test_images_part.pt',\n",
    "        't2_data_2d.mrd',\n",
    "    ),\n",
    ")  # r: retries\n",
    "\n",
    "dataset_train = torch.utils.data.TensorDataset(\n",
    "    torch.load(data_folder_tv / 'training_images_part.pt').unsqueeze(-3).unsqueeze(-3)\n",
    ")\n",
    "dataset_validation = torch.utils.data.TensorDataset(\n",
    "    torch.load(data_folder_tv / 'validation_images_part.pt').unsqueeze(-3).unsqueeze(-3)\n",
    ")\n",
    "dataset_test = torch.utils.data.TensorDataset(\n",
    "    torch.load(data_folder_tv / 'test_images_part.pt').unsqueeze(-3).unsqueeze(-3)\n",
    ")\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True, batch_size=1)\n",
    "dataloader_validation = torch.utils.data.DataLoader(dataset_validation, shuffle=False, batch_size=1)\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset_test, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We define some auxiliary functions that we will need for retrospectively simulating undersampled\n",
    "and noisy k-space data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show data preparation details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def normalize_kspace_data_and_image(\n",
    "    kdata: torch.Tensor,\n",
    "    target_image: torch.Tensor | None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor | None]:\n",
    "    \"\"\"Normalize k-space data and (possibly) target image.\"\"\"\n",
    "    factor = 1.0 / kdata.abs().std()\n",
    "    kdata *= factor\n",
    "    if target_image is not None:\n",
    "        target_image *= factor\n",
    "    return kdata, target_image\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    target_image: torch.Tensor, super_resolution_factor: float, noise_variance: float, seed: int = 0\n",
    ") -> tuple[torch.Tensor, mrpro.operators.LinearOperator, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Prepare data for training by simulating k-space data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_image : torch.Tensor\n",
    "        Target image to be used for simulation.\n",
    "    super_resolution_factor : float\n",
    "        Factor determining by how much the number of pixels is increased (i.e. how much smaller\n",
    "        each pixel will be).\n",
    "    noise_variance : float\n",
    "        Variance of the Gaussian noise to be added.\n",
    "    seed : float\n",
    "        seed of the random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing k-space data, forward operator adjoint reconstruction.\n",
    "    \"\"\"\n",
    "    # randomly choose trajectories to define the fourier operator\n",
    "    ny, nx = target_image.shape[-2:]\n",
    "    recon_matrix = mrpro.data.SpatialDimension(z=1, y=ny, x=nx)\n",
    "    encoding_matrix = mrpro.data.SpatialDimension(z=1, y=ny, x=nx)\n",
    "\n",
    "    n_k1, n_k0 = ny // super_resolution_factor, nx // super_resolution_factor\n",
    "\n",
    "    traj = mrpro.data.traj_calculators.KTrajectoryCartesian()(\n",
    "        n_k0=int(n_k0),\n",
    "        k0_center=int(n_k0 // 2),\n",
    "        k1_idx=torch.arange(-n_k1 // 2, n_k1 // 2)[..., None, None, :, None],\n",
    "        k1_center=0,\n",
    "        k2_idx=torch.tensor(0),\n",
    "        k2_center=0,\n",
    "    )\n",
    "\n",
    "    fourier_operator = mrpro.operators.FourierOp(\n",
    "        traj=traj,\n",
    "        recon_matrix=recon_matrix,\n",
    "        encoding_matrix=encoding_matrix,\n",
    "    )\n",
    "\n",
    "    (kdata,) = fourier_operator(target_image)\n",
    "\n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "    kdata = kdata + noise_variance * kdata.std() * torch.randn(\n",
    "        kdata.shape, dtype=kdata.dtype, device=kdata.device, generator=rng\n",
    "    )\n",
    "\n",
    "    kdata, target_image = normalize_kspace_data_and_image(kdata, target_image)  # type: ignore[assignment]\n",
    "\n",
    "    (adjoint_recon,) = fourier_operator.H(kdata)\n",
    "\n",
    "    return kdata, fourier_operator, adjoint_recon, target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define some functions for plotting images.\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show_images(\n",
    "    *images: torch.Tensor,\n",
    "    titles: list[str] | None = None,\n",
    "    cmap: str = 'grey',\n",
    "    clim: tuple[float, float] | None = None,\n",
    "    rotation_k: int = 0,\n",
    "    colorbar: bool = False,\n",
    "    # show_mse: bool = False,\n",
    "    show_ssim: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Plot images.\"\"\"\n",
    "    if clim is None:\n",
    "        clim = (0.0, 1.0)\n",
    "    n_images = len(images)\n",
    "    single_figsize = 2.5\n",
    "    fig, axes = plt.subplots(1, n_images, squeeze=False, figsize=(n_images * single_figsize, single_figsize))\n",
    "\n",
    "    image_reference = images[-1].cpu() if images[-1].is_cuda else images[-1]\n",
    "    if rotation_k != 0:\n",
    "        image_reference = image_reference.rot90(k=rotation_k, dims=(-2, -1))\n",
    "    for i in range(n_images):\n",
    "        image = images[i].cpu() if images[i].is_cuda else images[i]\n",
    "        if rotation_k != 0:\n",
    "            image = image.rot90(k=rotation_k, dims=(-2, -1))\n",
    "        im = axes[0, i].imshow(image.abs().squeeze(), cmap=cmap, clim=clim)\n",
    "        if titles:\n",
    "            axes[0, i].set_title(titles[i], fontsize=12)\n",
    "        if colorbar and i == n_images - 1:\n",
    "            fig.colorbar(im, ax=axes[0, i])\n",
    "\n",
    "        if show_ssim and i != n_images - 1:\n",
    "            ssim = mrpro.operators.functionals.SSIM(target=image_reference)\n",
    "            (ssim_value,) = ssim(image)\n",
    "            axes[0, i].text(\n",
    "                0.5,\n",
    "                0.12,\n",
    "                f'SSIM: {ssim_value.item():.2f}',\n",
    "                color='white',\n",
    "                fontsize=12,\n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='top',\n",
    "                transform=axes[0, i].transAxes,\n",
    "                bbox={\n",
    "                    'facecolor': 'black',\n",
    "                    'alpha': 0.8,\n",
    "                    'pad': 1,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        axes[0, i].set_axis_off()\n",
    "    fig.subplots_adjust(wspace=0.03)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Let us have a look at one example of the BrainWeb images. From a target image, we can create the k-space\n",
    "data by applying the forward operator and adding Gaussian noise. Then, we compute the adjoint reconstruction.\n",
    "The function `prepare_data` (defined further above) does all of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select sample id 6 (you can change this to any other id, but sample 6 is a good one,\n",
    "# as it has some nice contrast)\n",
    "import itertools\n",
    "\n",
    "sample_id = 6\n",
    "(image,) = next(itertools.islice(dataloader_test, sample_id, sample_id + 1))\n",
    "\n",
    "super_resolution_factor = 2.0\n",
    "noise_variance = 0.4\n",
    "\n",
    "kdata, fourier_operator, adjoint_recon, image = prepare_data(\n",
    "    image, super_resolution_factor=super_resolution_factor, noise_variance=noise_variance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show training details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "images_list = [adjoint_recon, image]\n",
    "titles = ['Adjoint', 'Ground Truth']\n",
    "show_images(\n",
    "    *images_list,\n",
    "    titles=titles,\n",
    "    show_ssim=True,\n",
    "    rotation_k=-1,\n",
    "    clim=(0, 0.8 * image.abs().max().item()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "We now define the unrolled PDHG network by instantiating the `AdaptiveTVNetwork2D` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(2025)\n",
    "lambda_map_network = ParameterMapNetwork2D(n_filters=128)\n",
    "n_iterations = 128\n",
    "\n",
    "recon_matrix = mrpro.data.SpatialDimension(z=1, y=image.shape[-2], x=image.shape[-1])\n",
    "encoding_matrix = mrpro.data.SpatialDimension(z=1, y=image.shape[-2], x=image.shape[-1])\n",
    "\n",
    "adaptive_tv_network = AdaptiveTVNetwork2D(\n",
    "    recon_matrix=recon_matrix,\n",
    "    encoding_matrix=encoding_matrix,\n",
    "    lambda_map_network=lambda_map_network,\n",
    "    n_iterations=n_iterations,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Let us have a look at the regularization parameter map and the corresponding TV-reconstruction\n",
    "prior to network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    adaptive_tv_network = adaptive_tv_network.cuda()\n",
    "    adjoint_recon = adjoint_recon.cuda()\n",
    "    kdata = kdata.cuda()\n",
    "    fourier_operator = fourier_operator.cuda()\n",
    "\n",
    "regularization_parameter_map_before_training = adaptive_tv_network.estimate_lambda_map(adjoint_recon).detach()\n",
    "\n",
    "pdhg_recon_before_training = (\n",
    "    adaptive_tv_network(adjoint_recon, kdata, fourier_operator, regularization_parameter_map_before_training)\n",
    "    .detach()\n",
    "    .cpu()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Let us see how we could train the unrolled PDHG network. We set up the optimizer with an appropriate learning rate,\n",
    "number of epochs and write a simple training loop, in which we present the network input-target pairs and update\n",
    "the network's parameters by gradient-descent.\n",
    "Training neural networks, especially those based on algorithm unrolling, is time consuming. We therefore\n",
    "only perform a few weight updates here to showcase that the MSE is reduced during training. If you want to train the\n",
    "network on some on other data, you can use the following code snippet as a template and increase the number of images\n",
    "used for training as well as the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {'params': adaptive_tv_network.lambda_map_network.cnn_block.parameters(), 'lr': 1e-5},\n",
    "        {'params': [adaptive_tv_network.lambda_map_network.t], 'lr': 1e-2},\n",
    "    ],\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 4\n",
    "validation_loss_values = []\n",
    "best_model = None\n",
    "\n",
    "n_training = 16\n",
    "n_validation = 8\n",
    "\n",
    "time_start = time()\n",
    "outer_bar = tqdm(range(n_epochs), desc='Epochs', disable=False)\n",
    "for epoch in outer_bar:\n",
    "    for sample_num, (image_,) in enumerate(\n",
    "        itertools.islice(dataloader_train, n_training)\n",
    "    ):  # here, we only use 16 images for training\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        kdata_, fourier_operator_, adjoint_recon_, image_ = prepare_data(\n",
    "            image_,\n",
    "            super_resolution_factor=super_resolution_factor,\n",
    "            noise_variance=noise_variance,\n",
    "            seed=sample_num * epoch,\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            kdata_ = kdata_.cuda()\n",
    "            adjoint_recon_ = adjoint_recon_.cuda()\n",
    "            fourier_operator_ = fourier_operator_.cuda()\n",
    "            image_ = image_.cuda()\n",
    "\n",
    "        pdhg_recon_ = adaptive_tv_network(adjoint_recon_, kdata_, fourier_operator_)\n",
    "\n",
    "        loss = torch.nn.functional.mse_loss(torch.view_as_real(pdhg_recon_), torch.view_as_real(image_))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for sample_num, (image_,) in enumerate(\n",
    "        itertools.islice(dataloader_validation, n_validation)\n",
    "    ):  # here, we only use 4 images for validation\n",
    "        kdata_, fourier_operator_, adjoint_recon_, image_ = prepare_data(\n",
    "            image_,\n",
    "            super_resolution_factor=super_resolution_factor,\n",
    "            noise_variance=noise_variance,\n",
    "            seed=sample_num,\n",
    "        )\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            kdata_ = kdata_.cuda()\n",
    "            adjoint_recon_ = adjoint_recon_.cuda()\n",
    "            fourier_operator_ = fourier_operator_.cuda()\n",
    "            image_ = image_.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pdhg_recon_ = adaptive_tv_network(adjoint_recon_, kdata_, fourier_operator_)\n",
    "            loss = torch.nn.functional.mse_loss(torch.view_as_real(pdhg_recon_), torch.view_as_real(image_))\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    loss_validation = running_loss / len(dataset_validation)\n",
    "    validation_loss_values.append(loss_validation)\n",
    "    outer_bar.set_postfix(val_loss=f'{loss_validation:.8f}')\n",
    "    if validation_loss_values[-1] <= min(validation_loss_values):\n",
    "        # store the weights if the validation loss is the smallest; this is\n",
    "        # to avoid possible overfitting here, since we are only using very few\n",
    "        # images\n",
    "        best_model = copy.deepcopy(adaptive_tv_network.state_dict())\n",
    "\n",
    "time_end = time() - time_start\n",
    "print(f'training time: {datetime.timedelta(seconds=time_end)}')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, n_epochs + 1), validation_loss_values, label='Validation Loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Mean Squared Error')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "VoilÃ ! As you we can see, the network seems to learn something. A proper exhaustive training would\n",
    "take too long. Therefore, we here also provide a pre-trained model that we\n",
    "can load and use for the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_tv_network.load_state_dict(torch.load(data_folder_tv / 'tv_model.pt', map_location='cpu'))\n",
    "\n",
    "regularization_parameter_map_trained = adaptive_tv_network.estimate_lambda_map(adjoint_recon).detach()\n",
    "pdhg_recon_regularization_parameter_trained = (\n",
    "    (adaptive_tv_network(adjoint_recon, kdata, fourier_operator, regularization_parameter_map_trained).detach().cpu())\n",
    "    .detach()\n",
    "    .cpu()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "show_images(\n",
    "    regularization_parameter_map_before_training[0, 0],\n",
    "    regularization_parameter_map_trained[0, 0].cpu(),\n",
    "    titles=[\n",
    "        'Regularization  Map \\n (Before Training)',\n",
    "        'Regularization  Map \\n (Trained)',\n",
    "    ],\n",
    "    cmap='inferno',\n",
    "    rotation_k=-1,\n",
    "    clim=(0.0, 0.5 * regularization_parameter_map_trained.abs().max().item()),\n",
    "    colorbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Also, let us check that the learned regularization parameter maps improved the reconstructions\n",
    "over the ones obtained without training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "lines_to_next_cell": 2,
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "show_images(\n",
    "    adjoint_recon[0, 0],\n",
    "    pdhg_recon_before_training[0, 0],\n",
    "    pdhg_recon_regularization_parameter_trained[0, 0].cpu(),\n",
    "    image[0, 0],\n",
    "    titles=[\n",
    "        'Input Image',\n",
    "        'Spatially Adaptive TV \\n (Before Training)',\n",
    "        'Spatially Adaptive TV \\n (After Training)',\n",
    "        'Ground Truth',\n",
    "    ],\n",
    "    cmap='grey',\n",
    "    show_ssim=True,\n",
    "    rotation_k=-1,\n",
    "    clim=(0.0, 0.9 * image.abs().max().item()),\n",
    "    colorbar=False,\n",
    ")\n",
    "\n",
    "# Additionally, let us also perform a quick line search to see what the best possible TV-reconstruction using a\n",
    "# scalar regularization parameter would be. Note that in practice, you would obviously not be able\n",
    "# to obtain this reconstruction since the target image is not available. However, the comparison\n",
    "# is useful to assess how much improvement one can expect to obtain when employing\n",
    "# spatially adaptive regularization parameter maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show line search details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def line_search(\n",
    "    regularization_parameters: torch.Tensor,\n",
    "    initial_image: torch.Tensor,\n",
    "    kdata: torch.Tensor,\n",
    "    forward_operator: mrpro.operators.LinearOperator,\n",
    "    target: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Perform a line search to pick the best scalar regularization parameter for TV.\"\"\"\n",
    "    reconstructions_list = [\n",
    "        adaptive_tv_network(initial_image, kdata, forward_operator, regularization_parameter)\n",
    "        for regularization_parameter in regularization_parameters\n",
    "    ]\n",
    "    mse_values = torch.tensor(\n",
    "        [\n",
    "            torch.nn.functional.mse_loss(torch.view_as_real(recon), torch.view_as_real(target))\n",
    "            for recon in reconstructions_list\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return mse_values, reconstructions_list[torch.argmin(torch.tensor(mse_values))]\n",
    "\n",
    "\n",
    "regularization_parameters = torch.linspace(1e-1, 3e-1, 16)\n",
    "\n",
    "mse_values, pdhg_recon_best_scalar = line_search(\n",
    "    regularization_parameters,\n",
    "    adjoint_recon,\n",
    "    kdata,\n",
    "    fourier_operator,\n",
    "    (image).cuda() if torch.cuda.is_available() else image,\n",
    ")\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(regularization_parameters, mse_values.cpu())\n",
    "ax.set_xlabel(r'Scalar Regularization Parameter Value $\\lambda$', fontsize=12)\n",
    "ax.set_ylabel(r'MSE (TV($\\lambda$),Target)', fontsize=12)\n",
    "ax.vlines(\n",
    "    x=regularization_parameters[torch.argmin(mse_values)].item(),\n",
    "    ymin=mse_values.min().item(),\n",
    "    ymax=mse_values.max().item(),\n",
    "    colors='red',\n",
    "    ls=':',\n",
    "    label=r'Best scalar $\\lambda>0$',\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Below, you can see the adjoint reconstruction, the PDHG reconstruction with the best scalar regularization\n",
    "parameter, the PDHG reconstruction with the spatially adaptive as well as the target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "lines_to_next_cell": 0,
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "show_images(\n",
    "    adjoint_recon,\n",
    "    pdhg_recon_best_scalar,\n",
    "    pdhg_recon_regularization_parameter_trained,\n",
    "    image,\n",
    "    titles=[\n",
    "        'Adjoint',\n",
    "        'TV with\\n' + r'Best Scalar $\\lambda$',\n",
    "        'TV with\\n' + r'Learned $\\boldsymbol{\\Lambda}_{\\theta}$-Map',\n",
    "        'Ground Truth',\n",
    "    ],\n",
    "    show_ssim=True,\n",
    "    rotation_k=-1,\n",
    "    clim=(0.0, 0.9 * image.abs().max().item()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Application to in-vivo data\n",
    "Finally, let us download some ultra low field scanner data, apply the pre-trained network and\n",
    "visualize the results to see if the network generalizes well to in-vivo data.\n",
    "\n",
    "kdata_scanner = mrpro.data.KData.from_file(\n",
    "    data_folder_tv / 't2_data_2d.mrd',\n",
    "    mrpro.data.traj_calculators.KTrajectoryCartesian(),\n",
    ")\n",
    "\n",
    "kdata_scanner.data, _ = normalize_kspace_data_and_image(kdata_scanner.data, None)\n",
    "\n",
    "direct_reconstruction = mrpro.algorithms.reconstruction.DirectReconstruction(kdata_scanner)\n",
    "low_resolution_adjoint_recon = direct_reconstruction(kdata_scanner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_low_resolution, nx_low_resolution = kdata_scanner.data.shape[-2], kdata_scanner.data.shape[-1]\n",
    "nz, ny_target, nx_target = (\n",
    "    1,\n",
    "    int(super_resolution_factor * ny_low_resolution),\n",
    "    int(super_resolution_factor * nx_low_resolution),\n",
    ")  # target resolution\n",
    "n_k1, n_k0 = ny_low_resolution, nx_low_resolution\n",
    "\n",
    "recon_matrix = encoding_matrix = mrpro.data.SpatialDimension(z=nz, y=ny_target, x=nx_target)\n",
    "\n",
    "traj = mrpro.data.traj_calculators.KTrajectoryCartesian()(\n",
    "    n_k0=int(n_k0),\n",
    "    k0_center=int(n_k0 // 2),\n",
    "    k1_idx=torch.arange(-n_k1 // 2, n_k1 // 2)[..., None, None, :, None],\n",
    "    k1_center=0,\n",
    "    k2_idx=torch.tensor(0),\n",
    "    k2_center=0,\n",
    ")\n",
    "\n",
    "fourier_operator_scanner = mrpro.operators.FourierOp(\n",
    "    recon_matrix=recon_matrix,\n",
    "    encoding_matrix=encoding_matrix,\n",
    "    traj=traj,\n",
    ")\n",
    "\n",
    "(super_resolution_adjoint_reco,) = fourier_operator_scanner.H(kdata_scanner.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    super_resolution_adjoint_reco = super_resolution_adjoint_reco.cuda()\n",
    "    kdata_scanner = kdata_scanner.cuda()\n",
    "    fourier_operator_scanner = fourier_operator_scanner.cuda()\n",
    "\n",
    "regularization_parameter_map_scanner_data = adaptive_tv_network.estimate_lambda_map(\n",
    "    super_resolution_adjoint_reco\n",
    ").detach()\n",
    "\n",
    "pdhg_recon_scanner_data = (\n",
    "    adaptive_tv_network(\n",
    "        super_resolution_adjoint_reco,\n",
    "        kdata_scanner.data,\n",
    "        fourier_operator_scanner,\n",
    "        regularization_parameter_map_scanner_data,\n",
    "    )\n",
    "    .detach()\n",
    "    .cpu()\n",
    ")\n",
    "\n",
    "show_images(\n",
    "    regularization_parameter_map_scanner_data[0, 0],\n",
    "    titles=['Predicted Regularization \\n Parameter Map'],\n",
    "    cmap='inferno',\n",
    "    rotation_k=-1,\n",
    "    clim=(0.0, 0.4 * regularization_parameter_map_scanner_data.abs().max().item()),\n",
    "    colorbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "show_images(\n",
    "    1 / super_resolution_factor * low_resolution_adjoint_recon.data.squeeze(),\n",
    "    pdhg_recon_scanner_data.squeeze(),\n",
    "    titles=[\n",
    "        'Ultra-Low Field\\n Adjoint Reconstruction',\n",
    "        'Ultra-Low Field\\n' + r'TV with $\\boldsymbol{\\Lambda}_{\\theta}$-Map',\n",
    "    ],\n",
    "    show_ssim=False,\n",
    "    rotation_k=-1,\n",
    "    clim=(0.0, 0.6 * super_resolution_adjoint_reco.abs().max().item()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Well done, we have successfully reconstructed an image with spatially varying regularization parameter\n",
    "maps for TV. ðŸŽ‰\n",
    "\n",
    "\n",
    "## Next steps\n",
    "As previously mentioned, you can also change network architecture to something more sophisticated.\n",
    "Do deeper/wider networks give more accurate results?\n",
    "Further, you can play around with the number of iterations used for unrolling PDHG at training time. How does this\n",
    "number of iterations influence the obtained lambda maps and the final reconstruction?\n",
    "Further, since PDHG is a convergent method, you can also let the number of iterations of PDHG go to\n",
    "infinity at test time."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "mystnb,tags,-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
