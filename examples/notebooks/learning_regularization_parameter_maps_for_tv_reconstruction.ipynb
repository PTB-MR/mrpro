{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PTB-MR/mrpro/blob/main/examples/notebooks/learning_regularization_parameter_maps_for_tv_reconstruction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec('mrpro'):\n",
    "    %pip install mrpro[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Learning spatially adaptive regularization parameter maps for total-variation (TV)-minimization reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this notebook, we demonstrate how the method presented in\n",
    "[[Kofler et al, SIIMS 2023](https://epubs.siam.org/doi/abs/10.1137/23M1552486)] can be implemented for\n",
    "2D MR reconstruction problems using MRpro. The method consists of two main blocks.\n",
    "1) The first block is a neural network architecture that estimates spatially adaptive regularization\n",
    "parameter maps from an input image, which are then used in 2).\n",
    "2) The second block corresponds to an unrolled unrolled Primal-Dual Hybrid Gradient (PDHG) algorithm\n",
    "[[Chambolle \\& Pock, JMIV 2011](https://doi.org/10.1007%2Fs10851-010-0251-1)] that\n",
    "reconstructs the image from the undersampled k-space data measurements assuming the regularization\n",
    "parameter maps to be fixed.\n",
    "\n",
    "The entire network can be trained end-to-end using MRpro, allowing to learn to\n",
    "estimate spatially adaptive regularization parameter maps from an input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The method\n",
    "In the TV-example, (see <project:tv_minimization_reconstruction_pdhg.ipynb>), you can see how to employ the primal\n",
    "dual hybrid gradient (PDHG - `mrpro.algorithms.optimizers.pdhg`) method\n",
    "[[Chambolle \\& Pock, JMIV 2011](https://doi.org/10.1007%2Fs10851-010-0251-1)]\n",
    "to solve the TV-minimization problem. There, for data acquired according to the usual model\n",
    "\n",
    "$ y = Ax_{\\mathrm{true}} + n, $\n",
    "\n",
    "where $A$ contains the Fourier transform and the coil sensitivity maps operator, etc, and $n$ is complex-valued\n",
    "Gaussian noise, the TV-minimization problem is given by\n",
    "\n",
    "$\\mathcal{F}_{\\lambda}(x) = \\frac{1}{2}||Ax - y||_2^2 + \\lambda \\| \\nabla x \\|_1, \\quad \\quad \\quad (1)$\n",
    "\n",
    "where $\\nabla$ is the discretized gradient operator and $\\lambda>0$ globally dictates the strength of the\n",
    "regularization. Clearly, having one global regularization parameter $\\lambda$ for the entire image is\n",
    "not optimal, as the image content can vary significantly across the image. Therefore, in this example,\n",
    "we aim to learn spatially adaptive regularization parameter maps $\\Lambda_{\\theta}$ from the input image to improve\n",
    "our TV-reconstruction. I.e., we are interested in estimating spatially adaptive regularization parameter maps by\n",
    "\n",
    "$\\Lambda_{\\theta}:=u_{\\theta}(x_0)$\n",
    "\n",
    "with a convolutional neural network $u_{\\theta}$ with trainable parameters $\\theta$ from an input image $x_0$,\n",
    "and then to consider the weighted TV-minimization problem\n",
    "\n",
    "$\\mathcal{F}_{\\Lambda_{\\theta}}(x) = \\frac{1}{2}||Ax - y||_2^2 +  \\| \\Lambda_{\\theta} \\nabla x \\|_1, \\quad \\quad (2)$\n",
    "\n",
    "where $\\Lambda_{\\theta}$ is voxel-wise strictly positive and locally regularizes the problem by\n",
    "differently weighting the gradient of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## The neural network for estimating the regularization parameter maps\n",
    "In this example, we use a simple convolutional neural network to estimate the regularization parameter maps.\n",
    "Obviously, more complex and sophisticated architectures can be employed as well. For example, in the work in\n",
    "[[Kofler et al, SIIMS 2023](https://epubs.siam.org/doi/abs/10.1137/23M1552486)],\n",
    "a U-Net [[Ronneberger et al, MICCAI 2015](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28)] was used.\n",
    "The network used here corresponds to a simple block of convolutional layers with leaky ReLU activations and a\n",
    "final softplus activation to ensure that the regularization parameter maps are strictly positive.\n",
    "The network is defined in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ParameterMapNetwork2D(torch.nn.Module):\n",
    "    r\"\"\"A simple network for estimating regularization parameter maps for TV-reconstruction.\"\"\"\n",
    "\n",
    "    def __init__(self, n_filters: int = 16) -> None:\n",
    "        r\"\"\"Initialize Adaptive TV Network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_filters\n",
    "            Number of filters to be applied in the convolutional layers of the network.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.beta_softplus = 10.0\n",
    "        self.cnn_block: torch.nn.Module = torch.nn.Sequential(\n",
    "            *[\n",
    "                torch.nn.InstanceNorm2d(1, affine=False, track_running_stats=False),\n",
    "                torch.nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Conv2d(in_channels=n_filters, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.Softplus(beta=self.beta_softplus),\n",
    "            ]\n",
    "        )\n",
    "        # raw parameter t; softplus is used to \"activate\" it and make it strictly positive\n",
    "        self.t = torch.nn.Parameter(torch.tensor([0.0], requires_grad=True))\n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Apply the network to estimate regularization parameter maps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image\n",
    "            the image from which the regularization parameter maps should be estimated.\n",
    "        \"\"\"\n",
    "        regularization_parameter_map = self.cnn_block(image)\n",
    "        regularization_parameter_map = (\n",
    "            torch.nn.functional.softplus(self.t, beta=self.beta_softplus) * regularization_parameter_map\n",
    "        )\n",
    "        return regularization_parameter_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## The unrolled PDHG algorithm network\n",
    "We now construct the second block, i.e. network that (approximately) solves the TV-minimization problem with spatially\n",
    "adaptive regularization parameter maps given in (2) by unrolling a finite number of iterations of PDHG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "```{note}\n",
    "To fully understand the mechanism of the network, we recommend to first have a look at the TV-example in\n",
    "<project:tv_minimization_reconstruction_pdhg.ipynb>, especially if you are not familiar with the PDHG algorithm\n",
    "or how to use it to solve the TV-minimization problem.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Put in simple words, the network takes the initial image, estimates the regularization\n",
    "parameter maps, and then sets up the TV-problem described in (2) within the \"forward\" of the network.\n",
    "The network then approximately solves the TV-problem using the PDHG algorithm\n",
    "and returns the reconstructed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import mrpro\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class AdaptiveTVNetwork2D(torch.nn.Module):\n",
    "    r\"\"\"Unrolled primal dual hybrid gradient with spatially adaptive regularization parameter maps for TV.\n",
    "\n",
    "    Solves the minimization problem\n",
    "\n",
    "        :math:`\\min_x \\frac{1}{2}\\| Ax - y\\|_2^2 + \\| \\Lambda_{\\theta} \\nabla x\\|_1`,\n",
    "    where :math:`A` is the forward linear operator, :math:`\\nabla` is the gradient operator,\n",
    "    and :math:`\\Lambda_{\\theta}` is a strictly positive regularization parameter map that is estimated from\n",
    "    an input image with a network :math:`u_{\\theta}` with trainable parameters :math:`\\theta`.\n",
    "\n",
    "    N.B. The entire network sticks to the convention of MRpro, i.e. we work with images and k-space data\n",
    "    of shape (other*, coils, z, y, x). However, because here showcase the method for 2D problems,\n",
    "    some processing steps are necessary within the forward method. In particular, we restrict this example,\n",
    "    to be used with z=1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        recon_matrix: mrpro.data.SpatialDimension[int],\n",
    "        encoding_matrix: mrpro.data.SpatialDimension[int],\n",
    "        lambda_map_network: ParameterMapNetwork2D,\n",
    "        n_iterations: int = 128,\n",
    "    ):\n",
    "        r\"\"\"Initialize Adaptive TV Network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        recon_matrix\n",
    "            image shape, i.e. the shape of the domain of the fourier operator.\n",
    "        encoding_matrix\n",
    "           k-space shape, i.e. the shape of the domain of the fourier operator\n",
    "        lambda_map_network\n",
    "            a network that predicts a regularization parameter map from the input image.\n",
    "        n_iterations\n",
    "            number of iterations for the unrolled primal dual hybrid gradient (PDHG) algorithm.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.recon_matrix = recon_matrix\n",
    "        self.encoding_matrix = encoding_matrix\n",
    "\n",
    "        self.lambda_map_network = lambda_map_network\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        finite_differences_operator = mrpro.operators.FiniteDifferenceOp(dim=(-2, -1), mode='forward')\n",
    "        gradient_operator = (\n",
    "            mrpro.operators.RearrangeOp('grad batch ... -> batch grad ... ') @ finite_differences_operator\n",
    "        )\n",
    "        self.gradient_operator = gradient_operator\n",
    "\n",
    "        self.g = mrpro.operators.functionals.ZeroFunctional()\n",
    "\n",
    "        # operator norm of the stacked operator K=[A, \\nabla]^T\n",
    "        stacked_operator_norm = 3.0  # analytically calculated\n",
    "        self.primal_stepsize = self.dual_stepsize = 0.95 * (1.0 / stacked_operator_norm)\n",
    "\n",
    "    def estimate_lambda_map(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Estimate regularization parameter map from image.\"\"\"\n",
    "        # The initial image is a coil-combined image. Since we only consider 2D problems\n",
    "        # here, z can be assumed to be always 1.\n",
    "        input_image = rearrange(image.abs(), '... 1 1 y x -> ... 1 y x')\n",
    "        regularization_parameter_map = self.lambda_map_network(input_image).unsqueeze(-3).unsqueeze(-3)\n",
    "        return regularization_parameter_map\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        initial_image: torch.Tensor,\n",
    "        kdata: torch.Tensor,\n",
    "        forward_operator: mrpro.operators.LinearOperator,\n",
    "        regularization_parameter: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Reconstruct image using an unrolled PDHG algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_image\n",
    "            initial guess of the solution of the TV problem.\n",
    "        kdata\n",
    "            k-space data tensor of the considered problem.\n",
    "        forward_operator\n",
    "            forward operator that maps the image to the k-space data.\n",
    "        regularization_parameter\n",
    "            regularization parameter to be used in the TV-functional. If set to None,\n",
    "            it is estimated by the lambda_map_network.\n",
    "            (can also be a single scalar)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Image reconstructed by the PDGH algorithm to solve the weighted TV problem.\n",
    "        \"\"\"\n",
    "        # if no regularization parameter map is provided, compute it with the network\n",
    "        if regularization_parameter is None:\n",
    "            regularization_parameter = self.estimate_lambda_map(initial_image)\n",
    "\n",
    "        f1 = 0.5 * mrpro.operators.functionals.L2NormSquared(target=kdata)\n",
    "        f2 = mrpro.operators.functionals.L1NormViewAsReal(weight=regularization_parameter)\n",
    "        f = mrpro.operators.ProximableFunctionalSeparableSum(f1, f2)\n",
    "\n",
    "        stacked_operator = mrpro.operators.LinearOperatorMatrix(((forward_operator,), (self.gradient_operator,)))\n",
    "\n",
    "        (solution,) = mrpro.algorithms.optimizers.pdhg(\n",
    "            f=f,\n",
    "            g=self.g,\n",
    "            operator=stacked_operator,\n",
    "            initial_values=(initial_image,),\n",
    "            max_iterations=self.n_iterations,\n",
    "            primal_stepsize=self.primal_stepsize,\n",
    "            dual_stepsize=self.dual_stepsize,\n",
    "            tolerance=0.0,\n",
    "        )\n",
    "        return solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Creating the training data\n",
    "In the following, we create some training data for the network. We use some images borrowed from the\n",
    "BrainWeb dataset [[Aubert-Broche et al, IEEE TMI 2006](https://ieeexplore.ieee.org/abstract/document/1717639)],\n",
    "which is a simulated MRI dataset and for which MRpro provides a simple interface to load the data.\n",
    "For simplicity, we here use some images that were generated by simulating the contrast using the\n",
    "inversion recovery signl model in MRpro.\n",
    "First, we start by defining some auxiliary functions that we will need for retrospectively simulating undersampled\n",
    "and noisy k-space data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_gaussian_noise(kdata: torch.Tensor, noise_variance: float, seed: int) -> torch.Tensor:\n",
    "    \"\"\"Corrupt given k-space data with additive Gaussian noise.\"\"\"\n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "    kdata = kdata + noise_variance * kdata.std() * torch.randn(\n",
    "        kdata.shape, dtype=kdata.dtype, device=kdata.device, generator=rng\n",
    "    )\n",
    "\n",
    "    return kdata\n",
    "\n",
    "\n",
    "def normalize_kspace_data_and_image(\n",
    "    kdata: torch.Tensor,\n",
    "    target_image: torch.Tensor | None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor | None]:\n",
    "    \"\"\"Normalize k-space data and (possibly) target image.\"\"\"\n",
    "    factor = 1 / kdata.abs().max()\n",
    "    kdata *= factor\n",
    "    if target_image is not None:\n",
    "        target_image *= factor\n",
    "    return kdata, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Further, we define a torch dataset that will be used for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Cartesian2D(torch.utils.data.Dataset):\n",
    "    r\"\"\"Dataset for 2D problems.\"\"\"\n",
    "\n",
    "    def __init__(self, images: torch.Tensor) -> None:\n",
    "        \"\"\"Parameters\n",
    "\n",
    "        ----------\n",
    "        images\n",
    "            images to be used in the dataset.\n",
    "            Should have the shape (others, coils, z, y, x) with z=1.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of images in the dataset.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Return the image indexed by idx.\"\"\"\n",
    "        image = self.images[idx]\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "We also define a function that prepares the data for training. It simulates the k-space data by applying the\n",
    "forward operator to the image and adds Gaussian noise. The function also computes the adjoint reconstruction\n",
    "and the pseudo-inverse solution, which can be used as input image to estimate the regularization parameter maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show data preparation details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_data(\n",
    "    target_image: torch.Tensor, n_coils: int, acceleration_factor: float, noise_variance: float, seed: int = 0\n",
    ") -> tuple[torch.Tensor, mrpro.operators.LinearOperator, torch.Tensor, torch.Tensor | None, torch.Tensor]:\n",
    "    \"\"\"Prepare data for training by simulating k-space data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    target_image : torch.Tensor\n",
    "        Target image to be used for simulation.\n",
    "    n_coils : int\n",
    "        Number of coils for the simulation.\n",
    "    acceleration_factor : float\n",
    "        Acceleration factor for undersampling.\n",
    "    noise_variance : float\n",
    "        Variance of the Gaussian noise to be added.\n",
    "    seed : float\n",
    "        seed of the random number generator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing k-space data, forward operator adjoint reconstruction,\n",
    "        possibly the pseudo-inverse solution (if n_coils>1) and the target image.\n",
    "    \"\"\"\n",
    "    # randomly choose trajectories to define the fourier operator\n",
    "    ny, nx = target_image.shape[-2:]\n",
    "    recon_matrix = mrpro.data.SpatialDimension(z=1, y=ny, x=nx)\n",
    "    encoding_matrix = mrpro.data.SpatialDimension(z=1, y=ny, x=nx)\n",
    "\n",
    "    traj = mrpro.data.traj_calculators.KTrajectoryCartesian().gaussian_variable_density(\n",
    "        encoding_matrix=encoding_matrix, acceleration=acceleration_factor, fwhm_ratio=0.6, seed=seed\n",
    "    )\n",
    "\n",
    "    fourier_operator = mrpro.operators.FourierOp(\n",
    "        traj=traj,\n",
    "        recon_matrix=recon_matrix,\n",
    "        encoding_matrix=encoding_matrix,\n",
    "    )\n",
    "\n",
    "    # generate simualated coil sensitivity maps\n",
    "    from mrpro.phantoms.coils import birdcage_2d\n",
    "\n",
    "    if n_coils > 1:\n",
    "        csm = birdcage_2d(n_coils, recon_matrix, relative_radius=0.8)\n",
    "        csm_operator = mrpro.operators.SensitivityOp(csm)\n",
    "        forward_operator = fourier_operator @ csm_operator\n",
    "    else:\n",
    "        forward_operator = fourier_operator\n",
    "\n",
    "    (kdata,) = forward_operator(target_image)\n",
    "    kdata = add_gaussian_noise(kdata, noise_variance=noise_variance, seed=seed)\n",
    "\n",
    "    kdata, target_image = normalize_kspace_data_and_image(kdata, target_image)  # type: ignore[assignment]\n",
    "\n",
    "    (adjoint_recon,) = forward_operator.H(kdata)\n",
    "\n",
    "    if n_coils > 1:\n",
    "        # compute an approximation of the pseudo-inverse solution\n",
    "        # Note that this in general only makes sense if the forward operator can be injective,\n",
    "        # i.e. we have multiple coils with acceleration factor < n_coils\n",
    "        (pseudo_inverse_solution,) = mrpro.algorithms.optimizers.cg(\n",
    "            forward_operator.gram, right_hand_side=adjoint_recon, initial_value=adjoint_recon, max_iterations=16\n",
    "        )\n",
    "        return kdata, forward_operator, adjoint_recon, pseudo_inverse_solution, target_image\n",
    "    else:\n",
    "        pseudo_inverse_solution = None\n",
    "    return kdata, forward_operator, adjoint_recon, pseudo_inverse_solution, target_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Download the data from Zenodo, create the datasets and dataloaders and define\n",
    "some auxiliary functions for displaying the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show download and dataloading details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import zenodo_get\n",
    "\n",
    "dataset = '15407890'\n",
    "\n",
    "tmp = tempfile.TemporaryDirectory()  # RAII, automatically cleaned up\n",
    "data_folder_tv = Path(tmp.name)\n",
    "zenodo_get.zenodo_get([dataset, '-r', 5, '-o', data_folder_tv])  # r: retries\n",
    "\n",
    "n_images = 28\n",
    "images = torch.load(data_folder_tv / 'brainweb_data.pt')[:n_images, ...]\n",
    "images = images.unsqueeze(-3).unsqueeze(-3)\n",
    "dataset_train = Cartesian2D(images[:20, ...])\n",
    "dataset_validation = Cartesian2D(images[20:, ...])\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True, batch_size=1)\n",
    "dataloader_validation = torch.utils.data.DataLoader(dataset_validation, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define some functions for plotting images.\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "\n",
    "def show_images(\n",
    "    *images: torch.Tensor,\n",
    "    titles: list[str] | None = None,\n",
    "    cmap: str = 'grey',\n",
    "    clim: tuple[float, float] | None = None,\n",
    "    rotate: bool = True,\n",
    "    rotatation_k: int = -2,\n",
    "    colorbar: bool = False,\n",
    "    show_mse: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Plot images.\"\"\"\n",
    "    if clim is None:\n",
    "        clim = (0.0, 1.0)\n",
    "    n_images = len(images)\n",
    "    fig, axes = plt.subplots(1, n_images, squeeze=False, figsize=(n_images * 3, 3))\n",
    "\n",
    "    image_reference = images[-1].cpu() if images[-1].is_cuda else images[-1]\n",
    "    if rotate:\n",
    "        image_reference = image_reference.rot90(k=rotatation_k, dims=(-2, -1))\n",
    "    for i in range(n_images):\n",
    "        image = images[i].cpu() if images[i].is_cuda else images[i]\n",
    "        if rotate:\n",
    "            image = image.rot90(k=rotatation_k, dims=(-2, -1))\n",
    "        im = axes[0, i].imshow(image.abs(), cmap=cmap, clim=clim)\n",
    "        if titles:\n",
    "            axes[0, i].set_title(titles[i])\n",
    "        if colorbar:\n",
    "            fig.colorbar(im, ax=axes[0, i])\n",
    "        if show_mse and i != n_images - 1:\n",
    "            mse = torch.nn.functional.mse_loss(torch.view_as_real(image), torch.view_as_real(image_reference)).item()\n",
    "            axes[0, i].text(\n",
    "                10, 25, f'MSE={mse:.2e}', fontsize=12, color='yellow', bbox={'facecolor': 'grey', 'boxstyle': 'round'}\n",
    "            )\n",
    "        axes[0, i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Let us have a look at one example of the BrainWeb images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_ = next(iter(dataloader_validation))[0]\n",
    "\n",
    "n_coils = 8\n",
    "acceleration_factor = 4.0\n",
    "noise_variance = 0.05\n",
    "\n",
    "kdata_, forward_operator_, adjoint_recon_, pseudo_inverse_solution_, image_ = prepare_data(\n",
    "    image_, n_coils=n_coils, acceleration_factor=acceleration_factor, noise_variance=noise_variance\n",
    ")\n",
    "if n_coils > 1:\n",
    "    assert pseudo_inverse_solution_ is not None\n",
    "    images_list = [adjoint_recon_.squeeze(), pseudo_inverse_solution_.squeeze(), image_.squeeze()]\n",
    "    titles = ['Adjoint', 'Pseudo-Inverse', 'Target']\n",
    "else:\n",
    "    images_list = [adjoint_recon_.squeeze(), image_.squeeze()]\n",
    "    titles = ['Adjoint', 'Target']\n",
    "show_images(\n",
    "    *images_list,\n",
    "    titles=titles,\n",
    "    show_mse=True,\n",
    "    clim=(0, 0.8 * image_.abs().max().item()),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "We now define the unrolled PDHG network by instantiating the `AdaptiveTVNetwork2D` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(2025)\n",
    "lambda_map_network = ParameterMapNetwork2D(n_filters=32)\n",
    "n_iterations = 128\n",
    "\n",
    "recon_matrix = mrpro.data.SpatialDimension(z=1, y=images.shape[-2], x=images.shape[-1])\n",
    "encoding_matrix = mrpro.data.SpatialDimension(z=1, y=images.shape[-2], x=images.shape[-1])\n",
    "\n",
    "adaptive_tv_network = AdaptiveTVNetwork2D(\n",
    "    recon_matrix=recon_matrix,\n",
    "    encoding_matrix=encoding_matrix,\n",
    "    lambda_map_network=lambda_map_network,\n",
    "    n_iterations=n_iterations,\n",
    ")\n",
    "\n",
    "if n_coils > 1:\n",
    "    assert pseudo_inverse_solution_ is not None\n",
    "    input_image_ = pseudo_inverse_solution_\n",
    "else:\n",
    "    input_image_ = adjoint_recon_\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    adaptive_tv_network = adaptive_tv_network.cuda()\n",
    "    input_image_ = input_image_.cuda()\n",
    "    kdata_ = kdata_.cuda()\n",
    "    forward_operator_ = forward_operator_.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    regularization_parameter_map_init = adaptive_tv_network.estimate_lambda_map(input_image_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "Let us now train the unrolled PDHG network. We set up the optimizer with an appropriate learning rate, choose a\n",
    "number of epochs and write a simple training loop, in which we present the network input-target pairs and update\n",
    "the network's parameters by gradient-descent.\n",
    "To obtain a somewhat decent estimate of the network parameters, training should take approximately 5-6 minutes\n",
    "on a GPU. If you have GPU and you need a break, start the training now, grab yourself a coffee and come back later\n",
    "to see your network trained from scratch.\n",
    "If you do not have a GPU, we provide a pre-trained model, which you can load and directly apply to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# cnn_block = cast(torch.nn.Module, adaptive_tv_network.lambda_map_network.cnn_block)\n",
    "# cnn_block =\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {'params': adaptive_tv_network.lambda_map_network.cnn_block.parameters(), 'lr': 1e-4},\n",
    "        {'params': [adaptive_tv_network.lambda_map_network.t], 'lr': 1e-2},\n",
    "    ],\n",
    "    weight_decay=1e-8,\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 10\n",
    "validation_loss_values = []\n",
    "best_model = None\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    # without a gpu, skip training. instead, load the pre-trained model downloaded from zenodo\n",
    "    adaptive_tv_network.load_state_dict(torch.load(data_folder_tv / 'tv_model.pt', map_location='cpu'))\n",
    "else:\n",
    "    time_start = time()\n",
    "    outer_bar = tqdm(range(n_epochs), desc='Epochs', disable=False)\n",
    "    for epoch in outer_bar:\n",
    "        for sample_num, image_ in enumerate(dataloader_train):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            kdata, forward_operator_undersampled, adjoint_recon, pseudo_inverse_solution, image = prepare_data(\n",
    "                image_,\n",
    "                n_coils=n_coils,\n",
    "                acceleration_factor=acceleration_factor,\n",
    "                noise_variance=noise_variance,\n",
    "                seed=sample_num * epoch,\n",
    "            )\n",
    "\n",
    "            initial_image = adjoint_recon if pseudo_inverse_solution is None else pseudo_inverse_solution\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                kdata = kdata.cuda()\n",
    "                initial_image = initial_image.cuda()\n",
    "                forward_operator_undersampled = forward_operator_undersampled.cuda()\n",
    "                image = image.cuda()\n",
    "\n",
    "            pdhg_recon = adaptive_tv_network(initial_image, kdata, forward_operator_undersampled)\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(torch.view_as_real(pdhg_recon), torch.view_as_real(image))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        running_loss = 0.0\n",
    "        for sample_num, image_ in enumerate(dataloader_validation):\n",
    "            kdata, forward_operator_undersampled, adjoint_recon, pseudo_inverse_solution, image = prepare_data(\n",
    "                image_,\n",
    "                n_coils=n_coils,\n",
    "                acceleration_factor=acceleration_factor,\n",
    "                noise_variance=noise_variance,\n",
    "                seed=sample_num,\n",
    "            )\n",
    "\n",
    "            initial_image = adjoint_recon if pseudo_inverse_solution is None else pseudo_inverse_solution\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                kdata = kdata.cuda()\n",
    "                initial_image = initial_image.cuda()\n",
    "                forward_operator_undersampled = forward_operator_undersampled.cuda()\n",
    "                image = image.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # pdhg_recon = adaptive_tv_network(pseudo_inverse_solution, csm, kdata, traj)\n",
    "                pdhg_recon = adaptive_tv_network(initial_image, kdata, forward_operator_undersampled)\n",
    "                loss = torch.nn.functional.mse_loss(torch.view_as_real(pdhg_recon), torch.view_as_real(image))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        loss_validation = running_loss / len(dataset_validation)\n",
    "        validation_loss_values.append(loss_validation)\n",
    "        outer_bar.set_postfix(val_loss=f'{loss_validation:.8f}')\n",
    "        if validation_loss_values[-1] <= min(validation_loss_values):\n",
    "            # store the weights if the validation loss is the smallest; this is\n",
    "            # to avoid possible overfitting here, since we are only using very few\n",
    "            # images\n",
    "            best_model = copy.deepcopy(adaptive_tv_network.state_dict())\n",
    "\n",
    "    time_end = time() - time_start\n",
    "    print(f'training time: {datetime.timedelta(seconds=time_end)}')\n",
    "\n",
    "    assert best_model is not None\n",
    "    adaptive_tv_network.load_state_dict(best_model)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(1, n_epochs + 1), validation_loss_values, label='Validation Loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "Nice! We have now trained our network for estimating the regularization parameter maps. Let us check if the obtained\n",
    "maps show interesting features by applying it to the previous image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    regularization_parameter_map_trained = adaptive_tv_network.estimate_lambda_map(input_image_)\n",
    "\n",
    "    pdhg_recon_regularization_parameter_trained_ = adaptive_tv_network(\n",
    "        input_image_, kdata_, forward_operator_, regularization_parameter_map_trained\n",
    "    )\n",
    "\n",
    "show_images(\n",
    "    regularization_parameter_map_init[0, 0].squeeze(),\n",
    "    regularization_parameter_map_trained[0, 0].squeeze().cpu(),\n",
    "    titles=[\n",
    "        'Reg. Parameter Map \\n (Before Training)',\n",
    "        'Reg. Parameter Map \\n (After Training)',\n",
    "    ],\n",
    "    cmap='inferno',\n",
    "    clim=(0.0, regularization_parameter_map_trained.abs().max().item()),\n",
    "    colorbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Application to in-vivo data\n",
    "Let us now download some real measured scanner data and apply our trained network. We will use the same dataset of the\n",
    "Cartesian reconstruction example <project:cartesian_reconstruction.ipynb>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show download details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the raw Carestian MR data from zenodo\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import zenodo_get\n",
    "\n",
    "dataset = '14173489'\n",
    "\n",
    "tmp = tempfile.TemporaryDirectory()  # RAII, automatically cleaned up\n",
    "data_folder_cart_data = Path(tmp.name)\n",
    "zenodo_get.zenodo_get([dataset, '-r', 5, '-o', data_folder_cart_data])  # r: retries\n",
    "\n",
    "kdata_cartesian = mrpro.data.KData.from_file(\n",
    "    data_folder_cart_data / 'cart_t1.mrd',\n",
    "    mrpro.data.traj_calculators.KTrajectoryCartesian(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "From the measured data, we select a subset of the sampled k-space lines to create an undersampled k-space\n",
    "data. Then, we estimate the coil sensitivity maps using the Walsh method implemented in MRpro and obtain the initial\n",
    "reconstruction by approximately solving the normal equations using conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "nz, ny, nx = kdata_cartesian.data.shape[-3:]\n",
    "encoding_matrix_cartesian = mrpro.data.SpatialDimension(z=nz, y=ny, x=nx)\n",
    "n_k1 = kdata_cartesian.data.shape[-2]\n",
    "k1_center = n_k1 // 2\n",
    "\n",
    "ktraj_undersampled = mrpro.data.traj_calculators.KTrajectoryCartesian().gaussian_variable_density(\n",
    "    encoding_matrix=encoding_matrix_cartesian, acceleration=acceleration_factor, fwhm_ratio=0.6, seed=2025\n",
    ")\n",
    "\n",
    "k1_idx = ktraj_undersampled.ky.squeeze() + k1_center\n",
    "kdata_undersampled = kdata_cartesian.data[..., k1_idx.to(torch.int), :]\n",
    "\n",
    "fourier_operator_undersampled = mrpro.operators.FourierOp(\n",
    "    traj=ktraj_undersampled,\n",
    "    recon_matrix=kdata_cartesian.header.recon_matrix,\n",
    "    encoding_matrix=kdata_cartesian.header.encoding_matrix,\n",
    ")\n",
    "\n",
    "kdata_undersampled_, _ = normalize_kspace_data_and_image(kdata_undersampled, None)\n",
    "assert isinstance(kdata_undersampled_, torch.Tensor)\n",
    "kdata_undersampled = kdata_undersampled_\n",
    "\n",
    "\n",
    "(coil_images,) = fourier_operator_undersampled.H(kdata_undersampled)\n",
    "\n",
    "csm_undersampled = mrpro.algorithms.csm.walsh(coil_images.squeeze(0), smoothing_width=5).unsqueeze(0)\n",
    "\n",
    "forward_operator_undersampled = fourier_operator_undersampled @ mrpro.operators.SensitivityOp(csm_undersampled)\n",
    "(adjoint_recon_undersampled,) = forward_operator_undersampled.H(kdata_undersampled)\n",
    "\n",
    "(pseudo_inverse_solution_undersampled,) = mrpro.algorithms.optimizers.cg(\n",
    "    forward_operator_undersampled.gram,\n",
    "    right_hand_side=adjoint_recon_undersampled,\n",
    "    initial_value=adjoint_recon_undersampled,\n",
    "    max_iterations=16,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pseudo_inverse_solution_undersampled = pseudo_inverse_solution_undersampled.cuda()\n",
    "    forward_operator_undersampled = forward_operator_undersampled.cuda()\n",
    "    kdata_undersampled = kdata_undersampled.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Finally, let us apply the trained network to the undersampled data. We first estimate the regularization parameter map\n",
    "from the pseudo-inverse solution and then apply the unrolled PDHG network to obtain the final reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_tv_network.recon_matrix = kdata_cartesian.header.recon_matrix\n",
    "adaptive_tv_network.encoding_matrix = kdata_cartesian.header.encoding_matrix\n",
    "\n",
    "with torch.no_grad():\n",
    "    regularization_parameter_map_undersampled_data = adaptive_tv_network.estimate_lambda_map(\n",
    "        pseudo_inverse_solution_undersampled\n",
    "    )\n",
    "\n",
    "    pdhg_recon_regularization_parameter_trained_ = adaptive_tv_network(\n",
    "        pseudo_inverse_solution_undersampled,\n",
    "        kdata_undersampled,\n",
    "        forward_operator_undersampled,\n",
    "        regularization_parameter_map_undersampled_data,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Let us have a look at the reconstruced image as well as as the estimate regularization parameter\n",
    "map. Also, let us compare the reconstructed image to the one reconstructed from the fully-sampled k-space data using\n",
    "the iterative SENSE reconstruction class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create DirectReconstruction object from KData object. Also, scale the k-space data to better match the\n",
    "# intensities of the undersampled k-space data.\n",
    "\n",
    "fourier_operator_full = mrpro.operators.FourierOp(\n",
    "    traj=kdata_cartesian.traj,\n",
    "    recon_matrix=kdata_cartesian.header.recon_matrix,\n",
    "    encoding_matrix=kdata_cartesian.header.encoding_matrix,\n",
    ")\n",
    "\n",
    "kdata_cartesian_data_normalized, _ = normalize_kspace_data_and_image(kdata_cartesian.data, None)\n",
    "assert isinstance(kdata_cartesian_data_normalized, torch.Tensor)\n",
    "kdata_cartesian.data = kdata_cartesian_data_normalized\n",
    "iterative_sense_reconstruction = mrpro.algorithms.reconstruction.DirectReconstruction(kdata_cartesian)\n",
    "image_fully_sampled = iterative_sense_reconstruction(kdata_cartesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Additionally, we also perform a quick line search to see what the best possible TV-reconstruction using a\n",
    "scalar regularization parameter would be. Note that in practice, you would obviously not be able\n",
    "to obtain this reconstruction since the target image is not available. However, the comparison\n",
    "is useful to assess how much improvement one can expect to obtain when employing\n",
    "spatially adaptive regularization parameter maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(\n",
    "    regularization_parameters: torch.Tensor,\n",
    "    initial_image: torch.Tensor,\n",
    "    kdata: torch.Tensor,\n",
    "    forward_operator: mrpro.operators.LinearOperator,\n",
    "    target: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Perform a line search to pick the best regularization parameter for TV.\"\"\"\n",
    "    reconstructions_list = [\n",
    "        adaptive_tv_network(initial_image, kdata, forward_operator, regularization_parameter)\n",
    "        for regularization_parameter in regularization_parameters\n",
    "    ]\n",
    "    mse_values = torch.tensor(\n",
    "        [\n",
    "            torch.nn.functional.mse_loss(torch.view_as_real(recon), torch.view_as_real(target))\n",
    "            for recon in reconstructions_list\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return mse_values, reconstructions_list[torch.argmin(torch.tensor(mse_values))]\n",
    "\n",
    "\n",
    "regularization_parameters = torch.linspace(0.0005, 0.0025, 10)\n",
    "mse_values, pdhg_recon_best_scalar = line_search(\n",
    "    regularization_parameters,\n",
    "    pseudo_inverse_solution_undersampled,\n",
    "    kdata_undersampled,\n",
    "    forward_operator_undersampled,\n",
    "    (image_fully_sampled.data).cuda() if torch.cuda.is_available() else image_fully_sampled.data,\n",
    ")\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(regularization_parameters, mse_values.cpu())\n",
    "ax.set_xlabel(r'Scalar Regularization Parameter Value $\\lambda$', fontsize=12)\n",
    "ax.set_ylabel(r'MSE (TV($\\lambda$),Target)', fontsize=12)\n",
    "ax.vlines(\n",
    "    x=regularization_parameters[torch.argmin(mse_values)].item(),\n",
    "    ymin=mse_values.min().item(),\n",
    "    ymax=mse_values.max().item(),\n",
    "    colors='red',\n",
    "    ls=':',\n",
    "    label=r'Best scalar $\\lambda>0$',\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.4f'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Finally, let us compare the different reconstructions. We show the adjoint reconstruction, the pseudo-inverse, the\n",
    "PDHG reconstruction with the best scalar regularization parameter, the PDHG reconstruction with the spatially adaptive\n",
    "regularization parameter map and the iterative SENSE reconstruction from the fully-sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(\n",
    "    adjoint_recon_undersampled.squeeze(),\n",
    "    pseudo_inverse_solution_undersampled.squeeze(),\n",
    "    pdhg_recon_best_scalar.squeeze(),\n",
    "    pdhg_recon_regularization_parameter_trained_.squeeze(),\n",
    "    image_fully_sampled.data.squeeze(),\n",
    "    titles=[\n",
    "        'Adjoint',\n",
    "        'Pseudo-Inverse',\n",
    "        r'PDHG (Best $\\lambda>0$)',\n",
    "        r'PDHG (Trained $\\Lambda_{\\theta}$-Map)',\n",
    "        'Iterative SENSE \\n (Fully-Sampled)',\n",
    "    ],\n",
    "    show_mse=True,\n",
    "    clim=(0.0, 0.4 * image_fully_sampled.data.abs().max().item()),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_images(\n",
    "    regularization_parameter_map_undersampled_data[0, 0].squeeze(),\n",
    "    titles=[\n",
    "        r'$\\Lambda_{\\theta}$-Parameter Map',\n",
    "    ],\n",
    "    cmap='inferno',\n",
    "    clim=(0.0, regularization_parameter_map_trained.abs().max().item()),\n",
    "    colorbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Well done, we have successfully reconstructed an image with spatially varying regularization parameter\n",
    "maps for TV. ðŸŽ‰\n",
    "\n",
    "\n",
    "### Next steps\n",
    "As previously mentioned, you can also change network architecture to something more sophisticated.\n",
    "Do deeper/wider networks give more accurate results?\n",
    "Further, you can play around with the number of iterations used for unrolling PDHG at training time. How does this\n",
    "number of iterations influence the obtained lambda maps and the final reconstruction?\n",
    "Further, since PDHG is a convergent method, you can also let the number of iterations of PDHG go to\n",
    "infinity at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "mystnb,tags,-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
