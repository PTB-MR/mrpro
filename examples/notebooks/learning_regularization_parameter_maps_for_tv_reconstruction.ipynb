{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PTB-MR/mrpro/blob/main/examples/notebooks/learning_regularization_parameter_maps_for_tv_reconstruction.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec('mrpro'):\n",
    "    %pip install mrpro[notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Learning spatially adaptive regularization parameter maps for total-variation (TV)-minimization reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this notebook, we demonstrate how the method presented in\n",
    "[[Kofler et al, SIIMS 2023](https://epubs.siam.org/doi/abs/10.1137/23M1552486)] can be implemented for\n",
    "2D MR reconstruction problems using MRpro. The method consists of two main blocks.\n",
    "1) The first block is a neural network architecture that estimates spatially adaptive regularization\n",
    "parameter maps, which are then used in 2).\n",
    "2) The second block corresponds to an unrolled unrolled Primal-Dual Hybrid Gradient (PDHG) algorithm\n",
    "[[Chambolle \\& Pock, JMIV 2011](https://doi.org/10.1007%2Fs10851-010-0251-1)] that\n",
    "reconstruct the image from the undersampled k-space data measurements assuming the regularization\n",
    "parameter maps to be fixed.\n",
    "\n",
    "The entire network can be trained end-to-end using the MRpro framework, allowing to learn to\n",
    "estimate spatially adaptive regularization parameter maps from an input image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### The method\n",
    "In the TV-example, (see <project:tv_minimization_reconstruction_pdhg.ipynb>), you can see how to employ the primal\n",
    "dual hybrid gradient (PDHG) method [[Chambolle \\& Pock, JMIV 2011](https://doi.org/10.1007%2Fs10851-010-0251-1)]\n",
    "to solve the TV-minimization problem. There, for data acquired according to the usual model\n",
    "\n",
    "$ y = Ax_{\\mathrm{true}} + n, $\n",
    "\n",
    "where $A$ contains the Fourier transform and the coil sensitivity maps operator, etc, and $n$ is complex-valued\n",
    "Gaussian noise, the TV-minimization problem is given by\n",
    "\n",
    "$\\mathcal{F}_{\\lambda}(x) = \\frac{1}{2}||Ax - y||_2^2 + \\lambda \\| \\nabla x \\|_1, \\quad \\quad \\quad (1)$\n",
    "\n",
    "where $\\nabla$ is the discretized gradient operator and $\\lambda>0$ globally dictates the strength of the\n",
    "regularization. Clearly, having one global regularization parameter $\\lambda$ for the entire image is\n",
    "not optimal, as the image content can vary significantly across the image. Therefore, in this example,\n",
    "we aim to learn spatially adaptive regularization parameter maps $\\Lambda_{\\theta}$ from the input image to improve\n",
    "our TV-reconstruction. I.e., we are interested in estimating spatially adaptive regularization parameter maps by\n",
    "\n",
    "$\\Lambda_{\\theta}:=u_{\\theta}(x_0)$\n",
    "\n",
    "with a convolutional neural network $u_{\\theta}$ with trainable parameters $\\theta$ from an input image $x_0$,\n",
    "and then to consider the problem\n",
    "\n",
    "$\\mathcal{F}_{\\Lambda_{\\theta}}(x) = \\frac{1}{2}||Ax - y||_2^2 +  \\| \\Lambda_{\\theta} \\nabla x \\|_1, \\quad \\quad (2)$\n",
    "\n",
    "where $\\Lambda_{\\theta}$ is voxel-wise strictly positive and locally regularizes the TV-minimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### The neural network\n",
    "In this simple example, we use a simple convolutional neural network to estimate the regularization parameter maps.\n",
    "Obviously, more complex architectures can be employed as well. For example, in the work in\n",
    "[[Kofler et al, SIIMS 2023](https://epubs.siam.org/doi/abs/10.1137/23M1552486)],\n",
    "a U-Net [[Ronneberger et al, MICCAI 2015](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28)] was used.\n",
    "The network used here corresponds to a simple block of convolutional layers with leaky ReLU activations and a\n",
    "final softplus activation to ensure that the regularization parameter maps are strictly positive.\n",
    "The network is defined in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ParameterMapNetwork2D(torch.nn.Module):\n",
    "    r\"\"\"A simple network for estimating regularization parameter maps for TV-reconstruction.\"\"\"\n",
    "\n",
    "    def __init__(self, n_filters: int = 16) -> None:\n",
    "        r\"\"\"Initialize Adaptive TV Network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_filters\n",
    "            number of filters to be applied in the convolutional layers of the network.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn_block = torch.nn.Sequential(\n",
    "            *[\n",
    "                torch.nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Conv2d(in_channels=n_filters, out_channels=n_filters, kernel_size=3, stride=1, padding=1),\n",
    "                torch.nn.LeakyReLU(),\n",
    "                torch.nn.Conv2d(in_channels=n_filters, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
    "            ]\n",
    "        )\n",
    "        # raw parameter t, softplus is used to \"activate\" it and make it positive\n",
    "        self.t = torch.nn.Parameter(torch.tensor([-5.0], requires_grad=True))\n",
    "\n",
    "        self.beta_softplus = 1.0\n",
    "\n",
    "    def forward(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        r\"\"\"Apply the network to estimate regularization parameter maps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image\n",
    "            the image from which the regularization parameter maps should be estimated.\n",
    "\n",
    "        \"\"\"\n",
    "        dims = tuple(dim for dim in range(1, image.ndim))\n",
    "        image = (image - image.mean(dim=dims, keepdim=True)) / image.std(dim=dims, keepdim=True)\n",
    "        regularization_parameter_map = self.cnn_block(image)\n",
    "\n",
    "        # stack the parameter map channel dimension to share the regularization\n",
    "        # between the x- and y-direction of the image gradients\n",
    "        regularization_parameter_map = torch.concat(2 * [regularization_parameter_map], dim=1)\n",
    "\n",
    "        # apply softplus to enforce strict positvity and scale by hand-crafted parameter\n",
    "        regularization_parameter_map = torch.nn.functional.softplus(\n",
    "            self.t, beta=self.beta_softplus\n",
    "        ) * torch.nn.functional.softplus(regularization_parameter_map, beta=self.beta_softplus)\n",
    "        return regularization_parameter_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### The unrolled PDHG algorithm network\n",
    "We now construct the second block, i.e. network that solves the TV-minimization problem with spatially\n",
    "adaptive regularization parameter maps given in (2) by unrolling a finite number of iteration of PDHG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "```{note}\n",
    "To fully understand the mechanics of the network, we recommend to have a look at the TV-example in\n",
    "<project:tv_minimization_reconstruction_pdhg.ipynb>.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Put in simple words, the network takes the initial image, estimates the regularization\n",
    "parameter maps, and then sets up the TV-problem presented in (2) within the \"forward\" of the network.\n",
    "The network then approximately solves the TV-problem using the PDHG algorithm\n",
    "and returns the reconstructed image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import mrpro\n",
    "\n",
    "\n",
    "class AdaptiveTVNetwork2D(torch.nn.Module):\n",
    "    r\"\"\"Unrolled primal dual hybrid gradient with spatially adaptive regularization parameter maps for TV.\n",
    "\n",
    "    Solves the minimization problem\n",
    "\n",
    "        :math:`\\min_x \\frac{1}{2}\\| Ax - y\\|_2^2 + \\| \\Lambda_{\\theta} \\nabla x\\|_1`,\n",
    "    where :math:`A` is the forward linear operator, :math:`\\nabla` is the gradient operator,\n",
    "    and :math:`\\Lambda_{\\theta}` is a strictly positive regularization parameter map that is estimated from\n",
    "    an input image with a network :math:`u_{\\theta}` with trainable parameters :math:`\\theta`.\n",
    "\n",
    "    N.B. The entire network sticks to the convention of MRpro, i.e. we work with images and k-space data\n",
    "    of shape (other*, coils, z, y, x). However, because here showcase the method for 2D problems,\n",
    "    some processing steps are necessary within the forward method. In particular, we restrict this example,\n",
    "    to be used with z=1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_shape: mrpro.data.SpatialDimension[int],\n",
    "        k_shape: mrpro.data.SpatialDimension[int],\n",
    "        lambda_map_network: torch.nn.Module,\n",
    "        n_iterations: int = 128,\n",
    "    ):\n",
    "        r\"\"\"Initialize Adaptive TV Network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        img_shape\n",
    "            image shape, i.e. the shape of the domain of the fourier operator.\n",
    "        k_shape\n",
    "           k-space shape, i.e. the shape of the domain of the fourier operator\n",
    "        lambda_map_network\n",
    "            a network that predicts a regularization parameter map from the input image.\n",
    "        n_iterations\n",
    "            number of iterations for the unrolled primal dual hybrid gradient (PDHG) algorithm.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_shape = img_shape\n",
    "        self.k_shape = k_shape\n",
    "\n",
    "        self.lambda_map_network = lambda_map_network\n",
    "        self.n_iterations = n_iterations\n",
    "\n",
    "        finite_differences_operator = mrpro.operators.FiniteDifferenceOp(dim=(-2, -1), mode='forward')\n",
    "        gradient_operator = (\n",
    "            mrpro.operators.RearrangeOp('grad batch ... -> batch grad ... ') @ finite_differences_operator\n",
    "        )\n",
    "        self.gradient_operator = gradient_operator\n",
    "\n",
    "        self.g = mrpro.operators.functionals.ZeroFunctional()\n",
    "\n",
    "        # operator norm of the stacked operator K=[A, \\nabla]^T\n",
    "        self.stacked_operator_norm = 3.0  # analytically calculated\n",
    "\n",
    "    def estimate_lambda_map(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Estimate regularization parameter map from image.\"\"\"\n",
    "        # squeeze the dimensions that are one. In particular, the initial image\n",
    "        # is a coil-combined image (i.e. coils=1) and because, we only consider 2D problems here,\n",
    "        # z can be assumed to be always 1.\n",
    "        # (other*, coils, z, y, x) -> (other*, y, x)\n",
    "        input_image = image.abs().squeeze(-3)\n",
    "        regularization_parameter_map = self.lambda_map_network(input_image).unsqueeze(-3).unsqueeze(-3)\n",
    "        return regularization_parameter_map\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        initial_image: torch.Tensor,\n",
    "        csm: torch.Tensor,\n",
    "        kdata: torch.Tensor,\n",
    "        traj: mrpro.data.KTrajectory,\n",
    "        regularization_parameter: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Reconstruct image using an unrolled PDHG algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_image\n",
    "            initial guess of the solution of the TV problem.\n",
    "        csm\n",
    "            coil sensitivity map tensor of the considered problem.\n",
    "        kdata\n",
    "            k-space data tensor of the considered problem.\n",
    "        traj\n",
    "            k-space trajectory tensor of the Fourier considered operator.\n",
    "        regularization_parameter\n",
    "            regularization parameter to be used in the TV-functional. If set to None,\n",
    "            it is estimated by the lambda_map_network.\n",
    "            (can also be a single scalar)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            Image reconstructed by the TV-minimization algorithm.\n",
    "        \"\"\"\n",
    "        # if no regularization parameter map is provided, compute it with the network\n",
    "        if regularization_parameter is None:\n",
    "            regularization_parameter = self.estimate_lambda_map(initial_image)\n",
    "\n",
    "        f_1 = 0.5 * mrpro.operators.functionals.L2NormSquared(target=kdata, divide_by_n=False)\n",
    "        f_2 = mrpro.operators.functionals.L1NormViewAsReal(weight=regularization_parameter, divide_by_n=False)\n",
    "        f = mrpro.operators.ProximableFunctionalSeparableSum(f_1, f_2)\n",
    "\n",
    "        fourier_operator = mrpro.operators.FourierOp(\n",
    "            recon_matrix=self.img_shape,\n",
    "            encoding_matrix=self.k_shape,\n",
    "            traj=traj,\n",
    "        )\n",
    "        csm_operator = mrpro.operators.SensitivityOp(csm)\n",
    "        forward_operator = fourier_operator @ csm_operator\n",
    "        stacked_operator = mrpro.operators.LinearOperatorMatrix(((forward_operator,), (self.gradient_operator,)))\n",
    "\n",
    "        primal_stepsize = dual_stepsize = 0.95 * 1.0 / self.stacked_operator_norm\n",
    "\n",
    "        (solution,) = mrpro.algorithms.optimizers.pdhg(\n",
    "            f=f,\n",
    "            g=self.g,\n",
    "            operator=stacked_operator,\n",
    "            initial_values=(initial_image,),\n",
    "            max_iterations=self.n_iterations,\n",
    "            primal_stepsize=primal_stepsize,\n",
    "            dual_stepsize=dual_stepsize,\n",
    "            tolerance=0.0,\n",
    "        )\n",
    "        return solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Creating the training data\n",
    "In the following, we create some training data for the network. We use some images borrowed from the\n",
    "BrainWeb dataset [[Aubert-Broche et al, IEEE TMI 2006](https://ieeexplore.ieee.org/abstract/document/1717639),\n",
    "which is a simulated MRI dataset and for which MRpro provides a simple interface to load the data.\n",
    "For simplicity, we here use some images that were generated by simulating the contrast using the\n",
    "inversion recovery signl model in MRpro.\n",
    "First, we start by defining some auxiliary functions that we will need for retrospectively simulating undersampled\n",
    "and noisy k-space data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_gaussian_noise(kdata: torch.Tensor, noise_variance: float, seed: int) -> torch.Tensor:\n",
    "    \"\"\"Corrupt given k-space data with additive Gaussian noise.\"\"\"\n",
    "    rng = torch.Generator().manual_seed(seed)\n",
    "    kdata = kdata + noise_variance * kdata.std() * torch.randn(\n",
    "        kdata.shape, dtype=kdata.dtype, device=kdata.device, generator=rng\n",
    "    )\n",
    "\n",
    "    return kdata\n",
    "\n",
    "\n",
    "def normalize_kspace_data_and_image(\n",
    "    kdata: torch.Tensor, target_image: torch.Tensor | None, factor: float = 100\n",
    ") -> tuple[torch.Tensor, torch.Tensor] | torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize k-space data and (possibly) target image.\n",
    "\n",
    "    Divide by std multiply by (an empirically set) factor.\n",
    "    \"\"\"\n",
    "    kdata_norm = torch.linalg.norm(kdata)\n",
    "\n",
    "    kdata /= kdata_norm\n",
    "    kdata *= factor\n",
    "    if target_image is not None:\n",
    "        target_image /= kdata_norm\n",
    "        target_image *= factor\n",
    "\n",
    "        return kdata, target_image\n",
    "    else:\n",
    "        return kdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Further, we define a torch dataset that will be used for training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Cartesian2D(torch.utils.data.Dataset):\n",
    "    r\"\"\"Dataset for 2D problems.\"\"\"\n",
    "\n",
    "    def __init__(self, images: torch.Tensor) -> None:\n",
    "        \"\"\"Parameters\n",
    "\n",
    "        ----------\n",
    "        images\n",
    "            images to be used in the dataset.\n",
    "            Should have the shape (others, coils, z, y, x) with z=1.\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "\n",
    "    def prepare_data(\n",
    "        self, image: torch.Tensor, n_coils: int, acceleration_factor: int, noise_variance: float, seed: int = 0\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, mrpro.data.KTrajectory, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Prepare data for training by simulating k-space data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : torch.Tensor\n",
    "            Input image to be used for simulation.\n",
    "        n_coils : int\n",
    "            Number of coils for the simulation.\n",
    "        acceleration_factor : float\n",
    "            Acceleration factor for undersampling.\n",
    "        noise_variance : float\n",
    "            Variance of the Gaussian noise to be added.\n",
    "        seed : float\n",
    "            seed of the random number generator.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            A tuple containing k-space data, coil sensitivity maps, k-space trajectories adjoint reconstruction,\n",
    "            pseudo-inverse solution and the image.\n",
    "        \"\"\"\n",
    "        # randomly choose trajectories to define the fourier operator\n",
    "        ny, nx = image.shape[-2:]\n",
    "        img_shape = mrpro.data.SpatialDimension(z=1, y=ny, x=nx)\n",
    "        k_shape = mrpro.data.SpatialDimension(z=1, y=ny, x=nx)\n",
    "\n",
    "        traj = mrpro.data.traj_calculators.KTrajectoryCartesian().gaussian_variable_density(\n",
    "            encoding_matrix=k_shape, acceleration=acceleration_factor, n_center=10, fwhm_ratio=0.6, seed=seed\n",
    "        )\n",
    "\n",
    "        fourier_operator = mrpro.operators.FourierOp(\n",
    "            traj=traj,\n",
    "            recon_matrix=img_shape,\n",
    "            encoding_matrix=k_shape,\n",
    "        )\n",
    "\n",
    "        # generate simualated coil sensitivity maps\n",
    "        from mrpro.phantoms.coils import birdcage_2d\n",
    "\n",
    "        with torch.no_grad():\n",
    "            csm = birdcage_2d(n_coils, img_shape, relative_radius=0.8)\n",
    "            csm_operator = mrpro.operators.SensitivityOp(csm)\n",
    "\n",
    "            forward_operator = fourier_operator @ csm_operator\n",
    "\n",
    "            (kdata,) = forward_operator(image)\n",
    "            kdata = add_gaussian_noise(kdata, noise_variance=noise_variance, seed=seed)\n",
    "\n",
    "            assert image is not None\n",
    "            kdata, image = normalize_kspace_data_and_image(kdata, image)\n",
    "\n",
    "            (adjoint_recon,) = forward_operator.H(kdata)\n",
    "\n",
    "            # compute an approximation of the pseudo-inverse solution\n",
    "            # Note that this in general only makes sense if the forward operator can be injective,\n",
    "            # i.e. we have multiple coils with acceleration factor < n_coils\n",
    "            (pseudo_inverse_solution,) = mrpro.algorithms.optimizers.cg(\n",
    "                forward_operator.gram, right_hand_side=adjoint_recon, initial_value=adjoint_recon, max_iterations=16\n",
    "            )\n",
    "\n",
    "        return kdata, csm, traj, adjoint_recon, pseudo_inverse_solution, image\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of images in the dataset.\"\"\"\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Return the image indexed by idx.\"\"\"\n",
    "        image = self.images[idx]\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Download the data from Zenodo and create the datasets and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import zenodo_get\n",
    "\n",
    "dataset = '15348116'\n",
    "\n",
    "tmp = tempfile.TemporaryDirectory()  # RAII, automatically cleaned up\n",
    "data_folder_tv = Path(tmp.name)\n",
    "zenodo_get.zenodo_get([dataset, '-r', 5, '-o', data_folder_tv])  # r: retries\n",
    "\n",
    "n_images = 28\n",
    "images = torch.load(data_folder_tv / 'brainweb_data.pt')[:n_images, ...]\n",
    "images = images.unsqueeze(-3).unsqueeze(-3)\n",
    "\n",
    "dataset_train = Cartesian2D(images[:20, ...])\n",
    "dataset_validation = Cartesian2D(images[20:, ...])\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True, batch_size=1)\n",
    "dataloader_validation = torch.utils.data.DataLoader(dataset_validation, shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show plotting details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define some functions for plotting images.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ioff()\n",
    "import torch\n",
    "\n",
    "\n",
    "def show_images(\n",
    "    *images: torch.Tensor,\n",
    "    titles: list[str] | None = None,\n",
    "    cmap: str = 'grey',\n",
    "    clim: tuple[float, float] | None = None,\n",
    "    rotate: bool = True,\n",
    "    colorbar: bool = False,\n",
    "    show_mse: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"Plot images.\"\"\"\n",
    "    if clim is None:\n",
    "        clim = (0.0, 1.0)\n",
    "    n_images = len(images)\n",
    "    fig, axes = plt.subplots(1, n_images, squeeze=False, figsize=(n_images * 3, 3))\n",
    "\n",
    "    image_reference = images[-1].cpu() if images[-1].is_cuda else images[-1]\n",
    "    if rotate:\n",
    "        image_reference = image_reference.rot90(k=-1, dims=(-2, -1))\n",
    "    for i in range(n_images):\n",
    "        image = images[i].cpu() if images[i].is_cuda else images[i]\n",
    "        if rotate:\n",
    "            image = image.rot90(k=-1, dims=(-2, -1))\n",
    "        im = axes[0, i].imshow(image.abs(), cmap=cmap, clim=clim)\n",
    "        if titles:\n",
    "            axes[0, i].set_title(titles[i])\n",
    "        if colorbar:\n",
    "            fig.colorbar(im, ax=axes[0, i])\n",
    "        if show_mse and i != n_images - 1:\n",
    "            mse = torch.nn.functional.mse_loss(torch.view_as_real(image), torch.view_as_real(image_reference)).item()\n",
    "            axes[0, i].text(\n",
    "                10, 25, f'MSE={mse:.2e}', fontsize=12, color='yellow', bbox={'facecolor': 'grey', 'boxstyle': 'round'}\n",
    "            )\n",
    "        axes[0, i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Let us have a look at one example of the BrainWeb images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "image_ = next(iter(dataloader_validation))[0]\n",
    "\n",
    "n_coils = 8\n",
    "noise_variance = 0.05\n",
    "acceleration_factor = 4\n",
    "kdata_, csm_, traj_, adjoint_recon_, pseudo_inverse_solution_, image_ = dataset_validation.prepare_data(\n",
    "    image_, n_coils=n_coils, acceleration_factor=acceleration_factor, noise_variance=noise_variance\n",
    ")\n",
    "\n",
    "show_images(\n",
    "    adjoint_recon_.squeeze(),\n",
    "    pseudo_inverse_solution_.squeeze(),\n",
    "    image_.squeeze(),\n",
    "    titles=['Adjoint', 'Pseudo-Inverse', 'Target'],\n",
    "    rotate=False,\n",
    "    show_mse=True,\n",
    "    clim=(0, 0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "We now define the unrolled PDHG network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(2025)\n",
    "lambda_map_network = ParameterMapNetwork2D(n_filters=32)\n",
    "n_iterations = 128\n",
    "\n",
    "img_shape = mrpro.data.SpatialDimension(z=1, y=images.shape[-2], x=images.shape[-1])\n",
    "k_shape = mrpro.data.SpatialDimension(z=1, y=images.shape[-2], x=images.shape[-1])\n",
    "\n",
    "adaptive_tv_network = AdaptiveTVNetwork2D(\n",
    "    img_shape=img_shape,\n",
    "    k_shape=k_shape,\n",
    "    lambda_map_network=lambda_map_network,\n",
    "    n_iterations=n_iterations,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    adaptive_tv_network = adaptive_tv_network.cuda()\n",
    "    pseudo_inverse_solution_ = pseudo_inverse_solution_.cuda()\n",
    "    csm_ = csm_.cuda()\n",
    "    kdata_ = kdata_.cuda()\n",
    "    traj_ = traj_.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    regularization_parameter_map_init = adaptive_tv_network.estimate_lambda_map(pseudo_inverse_solution_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Network training\n",
    "Let us now train the unrolled PDHG network. We set up the optimizer with an appropriate learning rate, choose a\n",
    "number of epochs and write a simple training loop, in which we present the network input-target pairs and update\n",
    "the network's parameters by gradient-descent.\n",
    "To obtain a somewhat decent estimate of the network parameters, training should take approximately 5-6 minutes\n",
    "on a GPU. If you have GPU and you need a break, start the training now, grab yourself a coffee and come back later\n",
    "to see your network trained from scratch.\n",
    "If you do not have a GPU, we provide a pre-trained model, which you can load and directly apply to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "cnn_block = cast(torch.nn.Module, adaptive_tv_network.lambda_map_network.cnn_block)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {'params': cnn_block.parameters(), 'lr': 1e-4},\n",
    "        {'params': [adaptive_tv_network.lambda_map_network.t], 'lr': 1e-1},\n",
    "    ],\n",
    "    weight_decay=1e-5,\n",
    ")\n",
    "\n",
    "import copy\n",
    "import datetime\n",
    "from time import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_epochs = 10\n",
    "validation_loss_values = []\n",
    "best_model = None\n",
    "noise_variance = 0.05\n",
    "acceleration_factor = 4\n",
    "\n",
    "time_start = time()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for epoch in tqdm(range(n_epochs), desc='epochs', disable=False):\n",
    "        for sample_num, image_ in enumerate(dataloader_train):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            kdata, csm, traj, adjoint_recon, pseudo_inverse_solution, image = dataset_train.prepare_data(\n",
    "                image_,\n",
    "                n_coils=n_coils,\n",
    "                acceleration_factor=acceleration_factor,\n",
    "                noise_variance=noise_variance,\n",
    "                seed=sample_num * epoch,\n",
    "            )\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                kdata = kdata.cuda()\n",
    "                traj = traj.cuda()\n",
    "                csm = csm.cuda()\n",
    "                pseudo_inverse_solution = pseudo_inverse_solution.cuda()\n",
    "                image = image.cuda()\n",
    "\n",
    "            pdhg_recon = adaptive_tv_network(pseudo_inverse_solution, csm, kdata, traj)\n",
    "\n",
    "            loss = torch.nn.functional.mse_loss(torch.view_as_real(pdhg_recon), torch.view_as_real(image))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        seed_validation = 0\n",
    "        running_loss = 0.0\n",
    "        for sample_num, image_ in enumerate(dataloader_validation):\n",
    "            kdata, csm, traj, adjoint_recon, pseudo_inverse_solution, image = dataset_validation.prepare_data(\n",
    "                image_,\n",
    "                n_coils=n_coils,\n",
    "                acceleration_factor=acceleration_factor,\n",
    "                noise_variance=noise_variance,\n",
    "                seed=sample_num,\n",
    "            )\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                kdata = kdata.cuda()\n",
    "                traj = traj.cuda()\n",
    "                csm = csm.cuda()\n",
    "                pseudo_inverse_solution = pseudo_inverse_solution.cuda()\n",
    "                image = image.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pdhg_recon = adaptive_tv_network(pseudo_inverse_solution, csm, kdata, traj)\n",
    "                loss = torch.nn.functional.mse_loss(torch.view_as_real(pdhg_recon), torch.view_as_real(image))\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        loss_validation = running_loss / len(dataset_validation)\n",
    "        validation_loss_values.append(loss_validation)\n",
    "        print(validation_loss_values[-1])\n",
    "        if validation_loss_values[-1] <= min(validation_loss_values):\n",
    "            # store the weights if the validation loss is the smallest; this is\n",
    "            # to avoid possible overfitting here, since we are only using very few\n",
    "            # images\n",
    "            best_model = copy.deepcopy(adaptive_tv_network.state_dict())\n",
    "\n",
    "    assert best_model is not None\n",
    "    adaptive_tv_network.load_state_dict(best_model)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(range(1, n_epochs + 1), validation_loss_values, label='Validation Loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Mean Squared Error')\n",
    "    ax.legend()\n",
    "\n",
    "    time_end = time() - time_start\n",
    "    print(f'training time: {datetime.timedelta(seconds=time_end)}')\n",
    "else:\n",
    "    adaptive_tv_network.load_state_dict(torch.load(data_folder_tv / 'tv_model.pt', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Nice! We have now trained our network for estimating the regularization parameter maps. Let us check if the obtained\n",
    "maps show interesting features by applying it to the previous image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    regularization_parameter_map_trained = adaptive_tv_network.estimate_lambda_map(pseudo_inverse_solution_)\n",
    "\n",
    "    pdhg_recon_regularization_parameter_trained_ = adaptive_tv_network(\n",
    "        pseudo_inverse_solution_, csm_, kdata_, traj_, regularization_parameter_map_trained\n",
    "    )\n",
    "\n",
    "show_images(\n",
    "    regularization_parameter_map_init[0, 0].squeeze(),\n",
    "    regularization_parameter_map_trained[0, 0].squeeze().cpu(),\n",
    "    titles=[\n",
    "        'Reg. Parameter Map \\n (Before Training)',\n",
    "        'Reg. Parameter Map \\n (After Training)',\n",
    "    ],\n",
    "    cmap='inferno',\n",
    "    clim=(0.0, regularization_parameter_map_trained.abs().max().item()),\n",
    "    rotate=False,\n",
    "    colorbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# Application to in-vivo data\n",
    "Let us now download some real measured scanner data and apply our trained network. We will use the same dataset of the\n",
    "Cartesian reconstruction example <project:cartesian_reconstruction.ipynb>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "mystnb": {
     "code_prompt_show": "Show download details"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the raw data from zenodo\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import zenodo_get\n",
    "\n",
    "dataset = '14173489'\n",
    "\n",
    "tmp = tempfile.TemporaryDirectory()  # RAII, automatically cleaned up\n",
    "data_folder_cart_data = Path(tmp.name)\n",
    "zenodo_get.zenodo_get([dataset, '-r', 5, '-o', data_folder_cart_data])  # r: retries\n",
    "\n",
    "kdata_cartesian = mrpro.data.KData.from_file(\n",
    "    data_folder_cart_data / 'cart_t1.mrd',\n",
    "    mrpro.data.traj_calculators.KTrajectoryCartesian(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "From the measured data, we select a subset of the sampled k-space lines to create an undersampled k-space\n",
    "data. Then, we estimate the coil sensitivity maps using the Walsh method implemented in MRpro and obtain the initial\n",
    "reconstruction by approximately solving the normal equations using conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "nz, ny, nx = kdata_cartesian.data.shape[-3:]\n",
    "k_shape_cartesian = mrpro.data.SpatialDimension(z=nz, y=ny, x=nx)\n",
    "n_k1 = kdata_cartesian.data.shape[-2]\n",
    "k1_center = n_k1 // 2\n",
    "\n",
    "ktraj_undersampled = mrpro.data.traj_calculators.KTrajectoryCartesian().gaussian_variable_density(\n",
    "    encoding_matrix=k_shape_cartesian, acceleration=acceleration_factor, n_center=10, fwhm_ratio=0.6, seed=2024\n",
    ")\n",
    "\n",
    "k1_idx = ktraj_undersampled.ky.squeeze() + k1_center\n",
    "kdata_undersampled = kdata_cartesian.data[..., k1_idx.to(torch.int), :]\n",
    "\n",
    "kdata_undersampled_ = normalize_kspace_data_and_image(kdata_undersampled, None)\n",
    "assert isinstance(kdata_undersampled_, torch.Tensor)\n",
    "kdata_undersampled = kdata_undersampled_\n",
    "\n",
    "fourier_operator_undersampled = mrpro.operators.FourierOp(\n",
    "    traj=ktraj_undersampled,\n",
    "    recon_matrix=kdata_cartesian.header.recon_matrix,\n",
    "    encoding_matrix=kdata_cartesian.header.encoding_matrix,\n",
    ")\n",
    "(coil_images,) = fourier_operator_undersampled.H(kdata_undersampled)\n",
    "\n",
    "csm_undersampled = mrpro.algorithms.csm.walsh(coil_images.squeeze(0), smoothing_width=5).unsqueeze(0)\n",
    "\n",
    "forward_operator = fourier_operator_undersampled @ mrpro.operators.SensitivityOp(csm_undersampled)\n",
    "(adjoint_recon_undersampled,) = forward_operator.H(kdata_undersampled)\n",
    "\n",
    "(pseudo_inverse_solution_undersampled,) = mrpro.algorithms.optimizers.cg(\n",
    "    forward_operator.gram,\n",
    "    right_hand_side=adjoint_recon_undersampled,\n",
    "    initial_value=adjoint_recon_undersampled,\n",
    "    max_iterations=16,\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    pseudo_inverse_solution_undersampled = pseudo_inverse_solution_undersampled.cuda()\n",
    "    adjoint_recon_undersampled = adjoint_recon_undersampled.cuda()\n",
    "    csm_undersampled = csm_undersampled.cuda()\n",
    "    ktraj_undersampled = ktraj_undersampled.cuda()\n",
    "    kdata_undersampled = kdata_undersampled.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Finally, let us apply the trained network to the undersampled data. We first estimate the regularization parameter map\n",
    "from the pseudo-inverse solution and then apply the unrolled PDHG network to obtain the final reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaptive_tv_network.img_shape = kdata_cartesian.header.recon_matrix\n",
    "adaptive_tv_network.k_shape = kdata_cartesian.header.encoding_matrix\n",
    "\n",
    "with torch.no_grad():\n",
    "    regularization_parameter_map_undersampled_data = adaptive_tv_network.estimate_lambda_map(\n",
    "        pseudo_inverse_solution_undersampled\n",
    "    )\n",
    "\n",
    "    pdhg_recon_regularization_parameter_trained_ = adaptive_tv_network(\n",
    "        pseudo_inverse_solution_undersampled,\n",
    "        csm_undersampled,\n",
    "        kdata_undersampled,\n",
    "        ktraj_undersampled,\n",
    "        regularization_parameter_map_undersampled_data,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Finally, let us have a look at the reconstruced image as well as as the estimate regularization parameter\n",
    "map. Also, let us compare the reconstructed image to the one reconstructed from the fully-sampled k-space data using\n",
    "the iterative SENSE reconstruction class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create DirectReconstruction object from KData object. Also, scale the k-space data to better match the\n",
    "# intensities of the undersampled k-space data\n",
    "kdata_cartesian_data_normalized = normalize_kspace_data_and_image(kdata_cartesian.data, None)\n",
    "assert isinstance(kdata_cartesian_data_normalized, torch.Tensor)\n",
    "kdata_cartesian.data = kdata_cartesian_data_normalized\n",
    "iterative_sense_reconstruction = mrpro.algorithms.reconstruction.DirectReconstruction(kdata_cartesian)\n",
    "image_fully_sampled = iterative_sense_reconstruction(kdata_cartesian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Additionally, we also perform a quick line search to see what the best possible TV-reconstruction using a\n",
    "scalar regularization parameter would be. Note that in practice, you would obviously not be able\n",
    "to obtain this reconstruction since the target image is not available. However, the comparison\n",
    "is useful to assess how much improvement one can expect to obtain when employing\n",
    "spatially adaptive regularization parameter maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(\n",
    "    regularization_parameters: torch.Tensor,\n",
    "    initial_image: torch.Tensor,\n",
    "    csm: torch.Tensor,\n",
    "    kdata: torch.Tensor,\n",
    "    traj: mrpro.data.KTrajectory,\n",
    "    target: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Perform a line search to pick the best regularization parameter for TV.\"\"\"\n",
    "    reconstructions_list = [\n",
    "        adaptive_tv_network(initial_image, csm, kdata, traj, regularization_parameter)\n",
    "        for regularization_parameter in regularization_parameters\n",
    "    ]\n",
    "    mse_values = torch.tensor(\n",
    "        [\n",
    "            torch.nn.functional.mse_loss(torch.view_as_real(recon), torch.view_as_real(target))\n",
    "            for recon in reconstructions_list\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return mse_values, reconstructions_list[torch.argmin(torch.tensor(mse_values))]\n",
    "\n",
    "\n",
    "regularization_parameters = torch.linspace(0.014, 0.022, 10)\n",
    "mse_values, pdhg_recon_best_scalar = line_search(\n",
    "    regularization_parameters,\n",
    "    pseudo_inverse_solution_undersampled,\n",
    "    csm_undersampled,\n",
    "    kdata_undersampled,\n",
    "    ktraj_undersampled,\n",
    "    (image_fully_sampled.data).cuda() if torch.cuda.is_available() else image_fully_sampled.data,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(regularization_parameters, mse_values.cpu())\n",
    "ax.set_xlabel(r'Scalar Regularization Parameter Value $\\lambda$', fontsize=12)\n",
    "ax.set_ylabel(r'MSE(TV($\\lambda$),Target)', fontsize=12)\n",
    "ax.vlines(\n",
    "    x=regularization_parameters[torch.argmin(mse_values)].item(),\n",
    "    ymin=mse_values.min().item(),\n",
    "    ymax=mse_values.max().item(),\n",
    "    colors='red',\n",
    "    ls=':',\n",
    "    label=r'Best scalar $\\lambda>0$',\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Finally, let us compare the different reconstructions. We show the adjoint reconstruction, the pseudo-inverse, the\n",
    "PDHG reconstruction with the best scalar regularization parameter, the PDHG reconstruction with the spatially adaptive\n",
    "regularization parameter map and the iterative SENSE reconstruction from the fully-sampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(\n",
    "    adjoint_recon_undersampled.squeeze(),\n",
    "    pseudo_inverse_solution_undersampled.squeeze(),\n",
    "    pdhg_recon_best_scalar.squeeze(),\n",
    "    pdhg_recon_regularization_parameter_trained_.squeeze(),\n",
    "    image_fully_sampled.data.squeeze(),\n",
    "    titles=[\n",
    "        'Adjoint',\n",
    "        'Pseudo-Inverse',\n",
    "        r'PDHG (Best $\\lambda>0$)',\n",
    "        r'PDHG (Trained $\\Lambda_{\\theta}$-Map)',\n",
    "        'Iterative SENSE \\n (Fully-Sampled)',\n",
    "    ],\n",
    "    rotate=False,\n",
    "    show_mse=True,\n",
    "    clim=(0.0, 1.0),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_images(\n",
    "    regularization_parameter_map_undersampled_data[0, 0].squeeze(),\n",
    "    titles=[\n",
    "        r'$\\Lambda_{\\theta}$-Parameter Map',\n",
    "    ],\n",
    "    cmap='inferno',\n",
    "    clim=(0.0, regularization_parameter_map_trained.abs().max().item()),\n",
    "    rotate=False,\n",
    "    colorbar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "Well done, we have successfully reconstructed an image with spatially varying regularization parameter\n",
    "maps for TV. ðŸŽ‰\n",
    "\n",
    "\n",
    "### Next steps\n",
    "As previously mentioned, you can also change network architecture to something more sophisticated.\n",
    "Do deeper/wider networks give more accurate results?\n",
    "Further, you can play around with the number of iterations used for unrolling PDHG at training time. How does this\n",
    "number of iterations influence the obtained lambda maps and the final reconstruction?\n",
    "Further, since PDHG is a convergent method, you can also let the number of iterations of PDHG go to\n",
    "infinity, at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "mystnb,tags,-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
