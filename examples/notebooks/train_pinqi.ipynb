{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/PTB-MR/mrpro/blob/main/examples/notebooks/train_pinqi.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "if not importlib.util.find_spec('mrpro'):\n",
    "    %pip install mrpro[notebooks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: D102, ANN201\n",
    "from collections.abc import Sequence\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import Any, Literal, TypedDict\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "import mrpro\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl  # type:ignore[import-not-found]\n",
    "import torch\n",
    "import torch.utils.data._utils\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint  # type:ignore[import-not-found]\n",
    "from pytorch_lightning.loggers import NeptuneLogger  # type:ignore[import-not-found]\n",
    "\n",
    "\n",
    "class BatchType(TypedDict):\n",
    "    \"\"\"Typehint for a batch of data.\"\"\"\n",
    "\n",
    "    kdata: mrpro.data.KData\n",
    "    csm: mrpro.data.CsmData\n",
    "    m0: torch.Tensor\n",
    "    t1: torch.Tensor\n",
    "    mask: torch.Tensor\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A brainweb based cartesian qMRI dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder: Path,\n",
    "        signalmodel: mrpro.operators.SignalModel,\n",
    "        n_images: int,\n",
    "        size: int,\n",
    "        acceleration: int,\n",
    "        n_coils: int,\n",
    "        max_noise: float,\n",
    "        orientation: Sequence[Literal['axial', 'coronal', 'sagittal']],\n",
    "        random: bool = True,\n",
    "    ):\n",
    "        \"\"\"Initialize the dataset.\"\"\"\n",
    "        if random:\n",
    "            augment = mrpro.phantoms.brainweb.augment(size=size)\n",
    "        else:\n",
    "            augment = mrpro.phantoms.brainweb.augment(\n",
    "                size=size,\n",
    "                max_random_shear=0,\n",
    "                max_random_rotation=0,\n",
    "                max_random_scaling_factor=0,\n",
    "                p_horizontal_flip=0,\n",
    "                p_vertical_flip=1.0,\n",
    "            )\n",
    "        self.phantom = mrpro.phantoms.brainweb.BrainwebSlices(\n",
    "            folder=folder,\n",
    "            what=('m0', 't1', 'mask'),\n",
    "            seed='index' if not random else 'random',\n",
    "            slice_preparation=augment,\n",
    "            orientation=orientation,\n",
    "        )\n",
    "        self.signalmodel = signalmodel\n",
    "        self.encoding_matrix = mrpro.data.SpatialDimension(1, size, size)\n",
    "        self.fov = mrpro.data.SpatialDimension(0.01, 0.25, 0.25)\n",
    "        self.acceleration = acceleration\n",
    "        self.n_coils = n_coils\n",
    "        self._random = random\n",
    "        self.max_noise = max_noise\n",
    "        self._n_images = n_images\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Get the length of the dataset.\"\"\"\n",
    "        return len(self.phantom)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"Get an item from the dataset.\"\"\"\n",
    "        phantom = self.phantom[index]\n",
    "        (images,) = self.signalmodel(phantom['m0'], phantom['t1'])\n",
    "        seed = int(torch.randint(0, 1000000, (1,))) if self._random else index\n",
    "\n",
    "        traj = mrpro.data.traj_calculators.KTrajectoryCartesian.gaussian_variable_density(\n",
    "            encoding_matrix=self.encoding_matrix,\n",
    "            seed=seed,\n",
    "            acceleration=self.acceleration,\n",
    "            fwhm_ratio=1.5,\n",
    "            n_center=12,\n",
    "            n_other=(self._n_images,),\n",
    "        )\n",
    "        header = mrpro.data.KHeader(\n",
    "            encoding_matrix=self.encoding_matrix,\n",
    "            recon_matrix=self.encoding_matrix,\n",
    "            recon_fov=self.fov,\n",
    "            encoding_fov=self.fov,\n",
    "        )\n",
    "\n",
    "        if isinstance(self.signalmodel, mrpro.operators.models.SaturationRecovery):\n",
    "            header.ti = self.signalmodel.saturation_time.tolist()\n",
    "        elif isinstance(self.signalmodel, mrpro.operators.models.InversionRecovery):\n",
    "            header.ti = self.signalmodel.ti.tolist()\n",
    "\n",
    "        fourier_op = mrpro.operators.FourierOp(self.encoding_matrix, self.encoding_matrix, traj)\n",
    "        if self.n_coils > 1:\n",
    "            csm_tensor = mrpro.phantoms.coils.birdcage_2d(self.n_coils, self.encoding_matrix)\n",
    "        else:\n",
    "            csm_tensor = torch.ones(1, 1, *self.encoding_matrix.zyx)\n",
    "        csm = mrpro.data.CsmData(csm_tensor, header)\n",
    "        images = einops.rearrange(images, 't y x -> t 1 1 y x')\n",
    "        (data,) = (fourier_op @ csm.as_operator())(images)\n",
    "        data = data + torch.randn_like(data) * torch.rand(1) * self.max_noise * data.std()\n",
    "        kdata = mrpro.data.KData(header, data, traj)\n",
    "        return {'kdata': kdata, 'csm': csm, **phantom}\n",
    "\n",
    "\n",
    "def collate_fn(batch: Any):  # noqa: ANN401\n",
    "    \"\"\"Join dataclasses to a batch.\"\"\"\n",
    "    return torch.utils.data._utils.collate.collate(\n",
    "        batch,\n",
    "        collate_fn_map={\n",
    "            mrpro.data.Dataclass: lambda batch, *, collate_fn_map: batch[0].stack(*batch[1:]),  # noqa: ARG005\n",
    "            **torch.utils.data._utils.collate.default_collate_fn_map,\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "class PINQI(torch.nn.Module):\n",
    "    \"\"\"PINQI model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        signalmodel: mrpro.operators.SignalModel,\n",
    "        constraints_op: mrpro.operators.ConstraintsOp | mrpro.operators.MultiIdentityOp,\n",
    "        parameter_is_complex: Sequence[bool],\n",
    "        n_images: int,\n",
    "        n_iterations: int,\n",
    "        n_features_parameter_net: Sequence[int],\n",
    "        n_features_image_net: Sequence[int],\n",
    "    ):\n",
    "        \"\"\"Initialize the PINQI model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.signalmodel = mrpro.operators.RearrangeOp('t batch ... -> batch t ...') @ signalmodel @ constraints_op\n",
    "        self.constraints_op = constraints_op\n",
    "        self._n_images = n_images\n",
    "        self._parameter_is_complex = parameter_is_complex\n",
    "        real_parameters = sum(1 for c in parameter_is_complex if c) + len(parameter_is_complex)\n",
    "        self.parameter_net = torch.compile(\n",
    "            mrpro.nn.nets.UNet(\n",
    "                n_dim=2,\n",
    "                n_channels_in=n_images * 2,\n",
    "                n_channels_out=real_parameters,\n",
    "                attention_depths=(-1, -2),\n",
    "                n_features=n_features_parameter_net,\n",
    "                cond_dim=128,\n",
    "            ),\n",
    "            dynamic=False,\n",
    "            fullgraph=True,\n",
    "        )\n",
    "        self.image_net = torch.compile(\n",
    "            mrpro.nn.nets.UNet(\n",
    "                n_dim=2,\n",
    "                n_channels_in=2,\n",
    "                n_channels_out=2,\n",
    "                attention_depths=(),\n",
    "                n_features=n_features_image_net,\n",
    "                cond_dim=128,\n",
    "            ),\n",
    "            dynamic=False,\n",
    "            fullgraph=True,\n",
    "        )\n",
    "        self.lambdas_raw = torch.nn.Parameter(torch.ones(n_iterations, 3))\n",
    "        self.softplus = torch.nn.Softplus(beta=5)\n",
    "        self.iteration_embedding = torch.nn.Embedding(n_iterations + 1, 128)\n",
    "\n",
    "        def objective_factory(\n",
    "            lambda_parameters: torch.Tensor,\n",
    "            image: torch.Tensor,\n",
    "            *parameter_reg: torch.Tensor,\n",
    "        ):\n",
    "            dc = mrpro.operators.functionals.L2NormSquared(image) @ self.signalmodel\n",
    "            reg = mrpro.operators.ProximableFunctionalSeparableSum(\n",
    "                *[mrpro.operators.functionals.L2NormSquared(r) for r in parameter_reg]\n",
    "            )\n",
    "            return dc + lambda_parameters * reg\n",
    "\n",
    "        self.nonlinear_solver = mrpro.operators.OptimizerOp(\n",
    "            objective_factory,\n",
    "            lambda _l, _i, *parameter_reg: parameter_reg,\n",
    "        )\n",
    "\n",
    "    def get_linear_solver(self, gram: mrpro.operators.LinearOperator):\n",
    "        def operator_factory(\n",
    "            lambda_image: torch.Tensor,\n",
    "            lambda_q: torch.Tensor,\n",
    "            *_,\n",
    "        ):\n",
    "            return gram + lambda_image + lambda_q\n",
    "\n",
    "        def rhs_factory(\n",
    "            lambda_image: torch.Tensor,\n",
    "            lambda_q: torch.Tensor,\n",
    "            image_reg: torch.Tensor,\n",
    "            signal: torch.Tensor,\n",
    "            zero_filled_image: torch.Tensor,\n",
    "        ):\n",
    "            return (zero_filled_image + lambda_image * image_reg + lambda_q * signal,)\n",
    "\n",
    "        return mrpro.operators.ConjugateGradientOp(\n",
    "            operator_factory=operator_factory,\n",
    "            rhs_factory=rhs_factory,\n",
    "        )\n",
    "\n",
    "    def get_parameter_reg(self, image: torch.Tensor, iteration: int = 0) -> tuple[torch.Tensor, ...]:\n",
    "        image = einops.rearrange(\n",
    "            torch.view_as_real(image),\n",
    "            'batch t 1 1 y x complex-> batch (t complex) y x',\n",
    "        )\n",
    "        cond = self.iteration_embedding(torch.tensor(iteration, device=image.device))[None]\n",
    "        parameters = self.parameter_net(image.contiguous(), cond=cond)\n",
    "        parameters = einops.rearrange(parameters, 'batch parameters y x-> parameters batch 1 1 y x')\n",
    "        i = 0\n",
    "        result = []\n",
    "        for is_complex in self._parameter_is_complex:\n",
    "            if is_complex:\n",
    "                result.append(torch.complex(parameters[i], parameters[i + 1]))\n",
    "                i += 2\n",
    "            else:\n",
    "                result.append(parameters[i])\n",
    "                i += 1\n",
    "        return tuple(result)\n",
    "\n",
    "    def get_image_reg(self, image: torch.Tensor, iteration: int = 0) -> torch.Tensor:\n",
    "        batch = image.shape[0]\n",
    "        image = einops.rearrange(\n",
    "            torch.view_as_real(image),\n",
    "            'batch t 1 1 y x complex-> (batch t) complex y x',\n",
    "        )\n",
    "        cond = self.iteration_embedding(torch.tensor(iteration, device=image.device))[None]\n",
    "        image = image + self.image_net(image.contiguous(), cond=cond)\n",
    "        image = einops.rearrange(image, '(batch t) complex y x-> batch t 1 1 y x complex', batch=batch)\n",
    "        return torch.view_as_complex(image.contiguous())\n",
    "\n",
    "    def forward(self, kdata: mrpro.data.KData, csm: mrpro.data.CsmData):\n",
    "        csm_op = csm.as_operator()\n",
    "        fourier_op = mrpro.operators.FourierOp.from_kdata(kdata)\n",
    "        acquisition_op = fourier_op @ csm_op\n",
    "        gram = acquisition_op.gram\n",
    "        (zero_filled_image,) = acquisition_op.H(kdata.data)\n",
    "        images = list(mrpro.algorithms.optimizers.cg(gram, zero_filled_image, max_iterations=2))\n",
    "        parameters = [self.get_parameter_reg(images[-1], 0)]\n",
    "        linear_solver = self.get_linear_solver(gram)\n",
    "\n",
    "        for i, (lambda_image, lambda_q, lambda_parameter) in enumerate(self.softplus(self.lambdas_raw)):\n",
    "            image_reg = self.get_image_reg(images[-1], i + 1)\n",
    "            (signal,) = self.signalmodel(*parameters[-1])\n",
    "            images.extend(linear_solver(lambda_image, lambda_q, image_reg, signal, zero_filled_image))\n",
    "            parameters_reg = self.get_parameter_reg(images[-1], i + 1)\n",
    "            parameters.append(self.nonlinear_solver(lambda_parameter, images[-1], *parameters_reg))\n",
    "        if self.constraints_op is not None:\n",
    "            parameters = [self.constraints_op(*p) for p in parameters]\n",
    "        return images, parameters\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    \"\"\"Data module for training the PINQI model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder: Path,\n",
    "        signalmodel: mrpro.operators.SignalModel,\n",
    "        n_images: int,\n",
    "        size: int = 192,\n",
    "        acceleration: int = 10,\n",
    "        n_coils: int = 8,\n",
    "        max_noise: float = 0.1,\n",
    "        orientation_train: Sequence[Literal['axial', 'coronal', 'sagittal']] = (\n",
    "            'axial',\n",
    "            'coronal',\n",
    "            'sagittal',\n",
    "        ),\n",
    "        orientation_val: Sequence[Literal['axial', 'coronal', 'sagittal']] = ('axial',),\n",
    "        batch_size: int = 16,\n",
    "        num_workers: int = 4,\n",
    "    ):\n",
    "        \"\"\"Initialize the data module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['signalmodel', 'folder', 'num_workers'])\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_dataset = Dataset(\n",
    "            folder=folder,\n",
    "            signalmodel=signalmodel,\n",
    "            n_images=n_images,\n",
    "            size=size,\n",
    "            acceleration=acceleration,\n",
    "            n_coils=n_coils,\n",
    "            max_noise=max_noise,\n",
    "            orientation=orientation_train,\n",
    "            random=True,\n",
    "        )\n",
    "        self.val_dataset = torch.utils.data.Subset(\n",
    "            Dataset(\n",
    "                folder=folder,\n",
    "                signalmodel=signalmodel,\n",
    "                n_images=n_images,\n",
    "                size=size,\n",
    "                acceleration=acceleration,\n",
    "                n_coils=n_coils,\n",
    "                max_noise=max_noise,\n",
    "                orientation=orientation_val,\n",
    "                random=False,\n",
    "            ),\n",
    "            list(range(30, 500, 20)),\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=False,\n",
    "            persistent_workers=self.num_workers > 0,\n",
    "            collate_fn=collate_fn,\n",
    "            worker_init_fn=lambda *_: torch.set_num_threads(1),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=4,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=False,\n",
    "            persistent_workers=self.num_workers > 0,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "\n",
    "class PinqiModule(pl.LightningModule):\n",
    "    \"\"\"Module for training the PINQI model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        signalmodel: mrpro.operators.SignalModel,\n",
    "        constraints_op: mrpro.operators.ConstraintsOp,\n",
    "        parameter_is_complex: Sequence[bool],\n",
    "        n_images: int,\n",
    "        n_iterations: int = 4,\n",
    "        n_features_parameter_net: Sequence[int] = (64, 128, 192, 256),\n",
    "        n_features_image_net: Sequence[int] = (32, 48, 64, 96),\n",
    "        lr: float = 3e-4,  # noqa: ARG002\n",
    "        weight_decay: float = 1e-3,  # noqa: ARG002\n",
    "        loss_weights: Sequence[float] = (0.2, 0.1, 0.1, 0.1, 0.8),\n",
    "    ):\n",
    "        \"\"\"Initialize the PINQI module.\"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['signalmodel', 'constraints_op'])\n",
    "        if len(loss_weights) != n_iterations + 1:\n",
    "            raise ValueError(f'loss_weights must be of length {n_iterations + 1} for {n_iterations} iterations')\n",
    "        signalmodel = deepcopy(signalmodel)\n",
    "        constraints_op = deepcopy(constraints_op)\n",
    "        self.pinqi = PINQI(\n",
    "            signalmodel=signalmodel,\n",
    "            constraints_op=constraints_op,\n",
    "            parameter_is_complex=parameter_is_complex,\n",
    "            n_images=n_images,\n",
    "            n_iterations=n_iterations,\n",
    "            n_features_parameter_net=n_features_parameter_net,\n",
    "            n_features_image_net=n_features_image_net,\n",
    "        )\n",
    "\n",
    "        self.validation_step_outputs: dict[str, list] = {}\n",
    "        self.baseline = Baseline(signalmodel, constraints_op, parameter_is_complex)\n",
    "\n",
    "    def forward(self, kdata: mrpro.data.KData, csm: mrpro.data.CsmData):\n",
    "        \"\"\"Apply the PINQI model to the data.\"\"\"\n",
    "        return self.pinqi(kdata, csm)\n",
    "\n",
    "    def loss(self, predictions: Sequence[torch.Tensor], batch: BatchType) -> torch.Tensor:\n",
    "        \"\"\"Compute the loss.\"\"\"\n",
    "        loss = torch.tensor(0.0, device=self.device)\n",
    "        target_m0, target_t1, mask = map(torch.squeeze, (batch['m0'], batch['t1'], batch['mask']))\n",
    "        for prediction, weight in zip(predictions, self.hparams.loss_weights, strict=False):\n",
    "            prediction_m0, prediction_t1 = map(torch.squeeze, prediction)\n",
    "            loss_t1 = torch.nn.functional.mse_loss(prediction_t1[mask], target_t1[mask])\n",
    "            loss_m0 = torch.nn.functional.mse_loss(\n",
    "                torch.view_as_real(prediction_m0[mask]),\n",
    "                torch.view_as_real(target_m0[mask]),\n",
    "            )\n",
    "            loss_outside = prediction_m0[~mask].abs().mean()\n",
    "            loss = loss + weight * (loss_t1 + 0.5 * loss_m0 + 0.1 * loss_outside)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch: BatchType, _batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Training step.\"\"\"\n",
    "        _images, parameters = self(batch['kdata'], batch['csm'])\n",
    "        loss = self.loss(parameters, batch)\n",
    "        self.log(\n",
    "            'train/loss',\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            sync_dist=True,\n",
    "            batch_size=len(batch['mask']),\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: BatchType, batch_idx: int) -> None:\n",
    "        \"\"\"Validate.\n",
    "\n",
    "        Needs to be adapted for other signal models than Saturation Recovery.\n",
    "        \"\"\"\n",
    "        _images, parameters = self(batch['kdata'], batch['csm'])\n",
    "        loss = self.loss(parameters, batch)\n",
    "\n",
    "        pred_m0, pred_t1 = parameters[-1]\n",
    "        target_t1, target_m0 = batch['t1'][:, None, None], batch['m0'][:, None, None]\n",
    "        mask = batch['mask']\n",
    "        batch_size = len(batch['mask'])\n",
    "        (ssim_t1,) = mrpro.operators.functionals.SSIM(target_t1, mask)(pred_t1)\n",
    "        (l1_t1,) = mrpro.operators.functionals.L1Norm(target_t1, mask)(pred_t1)\n",
    "        (l1_m0,) = mrpro.operators.functionals.L1Norm(target_m0, mask)(pred_m0)\n",
    "        self.log('val/ssim_t1', ssim_t1, on_epoch=True, sync_dist=True, batch_size=batch_size)\n",
    "        self.log('val/l1_t1', l1_t1, on_epoch=True, sync_dist=True, batch_size=batch_size)\n",
    "        self.log('val/l1_m0', l1_m0, on_epoch=True, sync_dist=True, batch_size=batch_size)\n",
    "        self.log('val/loss', loss, on_epoch=True, sync_dist=True, batch_size=batch_size)\n",
    "\n",
    "        if batch_idx == 0 and self.trainer.is_global_zero:\n",
    "            self.validation_step_outputs['target_t1'] = batch['t1'].cpu()\n",
    "            self.validation_step_outputs['pred_t1'] = pred_t1.cpu()\n",
    "            self.validation_step_outputs['pred_m0'] = pred_m0.cpu()\n",
    "            self.validation_step_outputs['target_m0'] = target_m0.cpu()\n",
    "            self.validation_step_outputs['mask'] = batch['mask'].cpu()\n",
    "            baseline_m0, baseline_t1 = self.baseline(batch['kdata'], batch['csm'])\n",
    "            self.validation_step_outputs['baseline_t1'] = baseline_t1.cpu()\n",
    "            self.validation_step_outputs['baseline_m0'] = baseline_m0.cpu()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Validate.\n",
    "\n",
    "        Needs to be adapted for other signal models than Saturation Recovery.\n",
    "        \"\"\"\n",
    "        if not self.trainer.is_global_zero:\n",
    "            return\n",
    "        outputs = self.validation_step_outputs\n",
    "\n",
    "        samples = len(outputs['mask'])\n",
    "        fig, axes = plt.subplots(4, samples, figsize=(4 * samples, 16), squeeze=False)\n",
    "\n",
    "        for i in range(samples):\n",
    "            self.result_plot(\n",
    "                outputs['target_t1'][i],\n",
    "                outputs['pred_t1'][i],\n",
    "                outputs['mask'][i],\n",
    "                axes[:, i],\n",
    "                outputs['baseline_t1'][i],\n",
    "                '$T_1$ (s)',\n",
    "            )\n",
    "        fig.suptitle(f'$T_1$ Epoch {self.current_epoch}')\n",
    "        self.logger.run['val/images/t1'].log(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "        fig, axes = plt.subplots(4, samples, figsize=(4 * samples, 12))\n",
    "        for i in range(samples):\n",
    "            self.result_plot(\n",
    "                outputs['target_m0'][i].abs(),\n",
    "                outputs['pred_m0'][i].abs(),\n",
    "                outputs['mask'][i],\n",
    "                axes[:, i],\n",
    "                outputs['baseline_m0'][i].abs(),\n",
    "                '$|M_0|$ (a.u.)',\n",
    "            )\n",
    "        fig.suptitle(f'$|M_0|$ Epoch {self.current_epoch}')\n",
    "        self.logger.run['val/images/m0'].log(fig)\n",
    "        plt.close(fig)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def result_plot(\n",
    "        self,\n",
    "        target: torch.Tensor,\n",
    "        pred: torch.Tensor,\n",
    "        mask: torch.Tensor,\n",
    "        axes: Sequence[plt.Axes],\n",
    "        baseline: torch.Tensor,\n",
    "        label: str,\n",
    "    ) -> None:\n",
    "        \"\"\"Plot the results.\"\"\"\n",
    "        target = target.squeeze().cpu()\n",
    "        pred = pred.squeeze().detach().cpu()\n",
    "        mask = mask.squeeze().detach().bool().cpu()\n",
    "        baseline = baseline.squeeze().detach().cpu()\n",
    "        target[~mask] = torch.nan\n",
    "        pred[~mask] = torch.nan\n",
    "        baseline[~mask] = torch.nan\n",
    "        difference = (target - pred) / target * 100\n",
    "        vmax = np.nanmax(target.numpy())\n",
    "\n",
    "        im0 = axes[0].imshow(target, vmin=0, vmax=vmax)\n",
    "        axes[0].set_title('Ground Truth')\n",
    "        axes[0].axis('off')\n",
    "        plt.colorbar(im0, ax=axes[0], fraction=0.046, pad=0.04, label=label)\n",
    "\n",
    "        im1 = axes[1].imshow(baseline, vmin=0, vmax=vmax)\n",
    "        axes[1].set_title('SENSE + Regression')\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im1, ax=axes[1], fraction=0.046, pad=0.04, label=label)\n",
    "\n",
    "        im2 = axes[2].imshow(pred, vmin=0, vmax=vmax)\n",
    "        axes[2].set_title('PINQI')\n",
    "        axes[2].axis('off')\n",
    "        plt.colorbar(im2, ax=axes[2], fraction=0.046, pad=0.04, label=label)\n",
    "\n",
    "        diff_vmax = np.nanpercentile(difference.abs().numpy(), 90)\n",
    "        im3 = axes[3].imshow(difference, cmap='coolwarm', vmin=-diff_vmax, vmax=diff_vmax)\n",
    "        axes[3].set_title('rel. Error')\n",
    "        axes[3].axis('off')\n",
    "        plt.colorbar(im3, ax=axes[3], fraction=0.046, pad=0.04, label='%')\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> dict:\n",
    "        \"\"\"Configure the optimizer and the learning rate scheduler.\"\"\"\n",
    "        scalars = ('lambdas_raw', 'rezero')\n",
    "        params, scalar_params = [], []\n",
    "        for n, p in self.named_parameters():\n",
    "            if not p.requires_grad:\n",
    "                continue\n",
    "            if any(s in n for s in scalars):\n",
    "                scalar_params.append(p)\n",
    "            else:\n",
    "                params.append(p)\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            [\n",
    "                {\n",
    "                    'params': params,\n",
    "                    'weight_decay': self.hparams.weight_decay,\n",
    "                    'lr': self.hparams.lr,\n",
    "                },\n",
    "                {\n",
    "                    'params': scalar_params,\n",
    "                    'weight_decay': 0.0,\n",
    "                    'lr': self.hparams.lr * 10,\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=[self.hparams.lr, 10 * self.hparams.lr],\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "            pct_start=0.1,\n",
    "            div_factor=20,\n",
    "            final_div_factor=300,\n",
    "        )\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'},\n",
    "        }\n",
    "\n",
    "\n",
    "class Baseline(torch.nn.Module):\n",
    "    \"\"\"Baseline solution using SENSE + Regression.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        signalmodel: mrpro.operators.SignalModel,\n",
    "        constraints_op: mrpro.operators.ConstraintsOp | mrpro.operators.MultiIdentityOp,\n",
    "        parameter_is_complex: Sequence[bool],\n",
    "    ):\n",
    "        \"\"\"Initialize the baseline.\"\"\"\n",
    "        super().__init__()\n",
    "        self.signalmodel = signalmodel\n",
    "        self.constraints_op = constraints_op\n",
    "        self.parameter_is_complex = parameter_is_complex\n",
    "\n",
    "    def forward(self, kdata: mrpro.data.KData, csm: mrpro.data.CsmData) -> tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Compute the baseline solution.\"\"\"\n",
    "        sense = mrpro.algorithms.reconstruction.RegularizedIterativeSENSEReconstruction(\n",
    "            kdata, csm=csm, regularization_weight=0.01, n_iterations=3\n",
    "        )\n",
    "        images = sense(kdata).rearrange('batch time ...-> time batch ...')\n",
    "\n",
    "        objective = mrpro.operators.functionals.L2NormSquared(images.data) @ self.signalmodel @ self.constraints_op\n",
    "        initial_values = tuple(\n",
    "            torch.zeros(\n",
    "                images.shape[1:],\n",
    "                device=images.device,\n",
    "                dtype=torch.complex64 if is_complex else torch.float32,\n",
    "            )\n",
    "            for is_complex in self.parameter_is_complex\n",
    "        )\n",
    "        solution = self.constraints_op(*mrpro.algorithms.optimizers.lbfgs(objective, initial_values))\n",
    "        return solution\n",
    "\n",
    "\n",
    "class LogLambdasCallback(pl.Callback):\n",
    "    \"\"\"Log the lambdas.\"\"\"\n",
    "\n",
    "    def on_train_batch_end(\n",
    "        self,\n",
    "        trainer: pl.Trainer,\n",
    "        pl_module: PinqiModule,\n",
    "        _outputs: dict,\n",
    "        _batch: BatchType,\n",
    "        _batch_idx: int,\n",
    "    ) -> None:\n",
    "        if trainer.global_step % 10 == 0:\n",
    "            lambdas = pl_module.pinqi.softplus(pl_module.pinqi.lambdas_raw).detach().cpu().numpy()\n",
    "            for iteration, (lambda_image, lambda_q, lambda_parameter) in enumerate(lambdas):\n",
    "                self.log_dict(\n",
    "                    {\n",
    "                        f'parameter/lambda_image_{iteration}': lambda_image,\n",
    "                        f'parameter/lambda_q_{iteration}': lambda_q,\n",
    "                        f'parameter/lambda_parameter_{iteration}': lambda_parameter,\n",
    "                    },\n",
    "                    on_step=True,\n",
    "                    on_epoch=False,\n",
    "                )\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    torch.set_float32_matmul_precision('high')\n",
    "    torch._inductor.config.compile_threads = 4\n",
    "    torch._inductor.config.worker_start_method = 'fork'\n",
    "    torch._dynamo.config.capture_scalar_outputs = True\n",
    "    torch._dynamo.config.cache_size_limit = 256\n",
    "    torch._functorch.config.activation_memory_budget = 0.5\n",
    "\n",
    "    data_folder = Path(' /echo/zimmer08/brainweb')\n",
    "    if not data_folder.exists():\n",
    "        data_folder.mkdir(parents=True, exist_ok=True)\n",
    "        mrpro.phantoms.brainweb.download_brainweb(output_directory=data_folder, workers=2, progress=True)\n",
    "\n",
    "    signalmodel = mrpro.operators.models.SaturationRecovery((0.2, 0.8, 4.0))\n",
    "    constraints_op = mrpro.operators.ConstraintsOp(\n",
    "        bounds=(\n",
    "            (-2, 2),  # M0 in [-2, 2]\n",
    "            (0.01, 6.0),  # T1 is constrained between 10 ms and 6 s\n",
    "        )\n",
    "    )\n",
    "    n_images = len(signalmodel.saturation_time)\n",
    "    parameter_is_complex = [True, False]\n",
    "\n",
    "    dm = DataModule(\n",
    "        folder=data_folder,\n",
    "        signalmodel=signalmodel,\n",
    "        n_images=n_images,\n",
    "        batch_size=8,\n",
    "        num_workers=8,\n",
    "        size=192,\n",
    "        acceleration=6,\n",
    "        n_coils=1,\n",
    "        max_noise=0.3,\n",
    "    )\n",
    "\n",
    "    model = PinqiModule(\n",
    "        signalmodel=signalmodel,\n",
    "        constraints_op=constraints_op,\n",
    "        parameter_is_complex=parameter_is_complex,\n",
    "        n_images=n_images,\n",
    "    )\n",
    "\n",
    "    neptune_logger = NeptuneLogger(\n",
    "        log_model_checkpoints=False,\n",
    "        dependencies='infer',\n",
    "    )\n",
    "    neptune_logger.log_model_summary(model=model, max_depth=-1)\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val/loss',\n",
    "        mode='min',\n",
    "        save_top_k=2,\n",
    "        dirpath=Path('checkpoints') / str(neptune_logger.version),\n",
    "        filename='{epoch:02d}-{val/loss:.4f}',\n",
    "        save_last=True,\n",
    "    )\n",
    "\n",
    "    strategy = 'auto'  # DDPStrategy(find_unused_parameters=False)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=100,\n",
    "        accelerator='gpu',\n",
    "        devices=1,\n",
    "        strategy=strategy,\n",
    "        logger=neptune_logger,\n",
    "        callbacks=[\n",
    "            LearningRateMonitor(logging_interval='step'),\n",
    "            checkpoint_callback,\n",
    "            LogLambdasCallback(),\n",
    "        ],\n",
    "        log_every_n_steps=10,\n",
    "        gradient_clip_algorithm='norm',\n",
    "        gradient_clip_val=5.0,\n",
    "    )\n",
    "\n",
    "    # trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "mystnb,tags,-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
